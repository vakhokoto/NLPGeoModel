{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob\n",
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize as tokenize\n",
    "import io, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import random\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "იმისათვის, რომ ჩავრვირთოთ ტორჩის მოდელი, საჭიროა რომ გავაკეთოთ მისი აღწერა:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "for ind, filename in enumerate(glob.glob(\"../input/data-splitting/train/*\")):\n",
    "    if ind % 2000 == 0:\n",
    "        print(ind, 'docs read')\n",
    "    fd = io.open(filename, mode=\"r\", encoding=\"utf-8\")\n",
    "    text = fd.read() \n",
    "    train.append(text)\n",
    "    fd.close()\n",
    "    \n",
    "train = train[:35000]\n",
    "\n",
    "train = [doc.replace('\"', ' \" ') for doc in train]\n",
    "train_tokens = [tokenize(doc) for doc in train]\n",
    "freqs = Counter()\n",
    "for doc in train_tokens:\n",
    "    freqs.update(doc)\n",
    "top_words = freqs.most_common()[:50000]\n",
    "vocab = ['UNK'] + [word for word, freq in top_words]\n",
    "\n",
    "word_to_ind = {word:ind for ind, word in enumerate(vocab)}\n",
    "ind_to_word = {ind:word for ind, word in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ჩავვირთოთ აწ უკვე დატრენინგებული მოდელი. მოდელი უკვე აღწერილია. საჭიროა მისი პარამეტრების დამთხვევა"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 90001 # the size of vocabulary\n",
    "emsize = 100 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.25 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "\n",
    "model.load_state_dict(torch.load('../resources/pytorch-transformer-model/best_model.pth', map_location=torch.device('cpu')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "აღვწეროთ ფუნქცია რომელიც მოცემუ წინადადებას გაუკეთებს ტოკენიზაციას"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sent(sequence):\n",
    "    \"\"\"\n",
    "    ფუნქცია რომელიც უკეთებს წინადადებას ტოკენიზაციას.\n",
    "    Args: str: წინადადება\n",
    "    Returns: აბრუნებს წინადადებების ტოკენების შესაბამის ინდექსებს\n",
    "    \"\"\"\n",
    "    seq = sequence.replace('\"', ' \" ')\n",
    "    seq = tokenize(seq)\n",
    "    tokenized = []\n",
    "    for word in seq:\n",
    "        tokenized.append(torch.tensor(word_to_ind.get(word, 0)))\n",
    "\n",
    "    data = torch.tensor(tokenized)\n",
    "    data = data.view(1, -1).t().contiguous()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "აღვწეროთ ფუნქცია, რომელიც გააგრძელებს მოცემულ წიანადადებას. ამ შემთხვევაში გაგრძელება მოხდება greedy და რანდომული არჩევანით."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_sent(sentence, seq_size):\n",
    "    \"\"\"\n",
    "    Args: sentence: გასაგრძელებელი წინადედება\n",
    "          seq_size: მაქსიმალური სიგრძე წინადადების\n",
    "          n: \n",
    "    \"\"\"\n",
    "    seq = tokenize_sent(sentence)\n",
    "    model.eval()\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            if seq.size(0) == seq_size: break\n",
    "            src_mask = model.generate_square_subsequent_mask(seq.size(0))\n",
    "            output = model(seq, src_mask)\n",
    "            output[-1][0][0] -= 500\n",
    "            mx = torch.argmax(output[-1][0]).item()\n",
    "            nxt = random.choices(list(range(90001)), output[-1][0].numpy())[0]\n",
    "            print(ind_to_word[mx])\n",
    "            seq = torch.cat((seq, torch.tensor(mx).view(1,1)), dim=0)\n",
    "            \n",
    "sentence = 'მსოფლიო საეკლესიო კრების'\n",
    "continue_sent(sentence, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ფუნქცია: რომელიც Beam Search-ით პულობს ყველაზე მაღალი შედეგის მქონდე N ცალ წინადადებას ხის pruning-ით "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_sent_beam(sentence, seq_size, n, max_len):\n",
    "    \"\"\"\n",
    "    Args: sentence: გასაგრძელებელი წინადედება\n",
    "          seq_size \n",
    "    \"\"\"\n",
    "    seq = tokenize_sent(sentence)\n",
    "    seqs = [(seq, 0)] * n\n",
    "    model.eval()\n",
    "    k = 0\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            # Pick top 2 items with highest score\n",
    "            seqs = sorted(seqs, reverse=True, key=lambda x: x[1])[:n]\n",
    "            print(seqs)\n",
    "            for seq, score in seqs:\n",
    "                if seq.size(0) == seq_size: break\n",
    "                src_mask = model.generate_square_subsequent_mask(seq.size(0))\n",
    "                output = model(seq, src_mask)\n",
    "                output[-1][0][0] -= 500\n",
    "                res, ind = torch.topk(output[-1][0], n)\n",
    "                new_seqs = []\n",
    "                for new_score, index in zip(res, ind):\n",
    "                    new_seq = torch.cat((seq, torch.tensor(index).view(1,1)), dim=0)\n",
    "                    new_seqs += [(new_seq, score+new_score)]\n",
    "#                     seq = torch.cat((seq, torch.tensor(mx).view(1,1)), dim=0)\n",
    "            seqs += new_seqs\n",
    "        k += 1\n",
    "        if k == max_len: break\n",
    "    return seqs\n",
    "            \n",
    "sentence = 'მარა პრინციპში '\n",
    "res = continue_sent_beam(sentence, 40, 2, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ბეჭდავს Beam Search-ით მიღებული შედეგის პასუხს სტრინგად"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_to_string(seq):\n",
    "    res = \"\"\n",
    "    for mx in seq[0]:\n",
    "        print(mx)\n",
    "        res +=  ind_to_word[mx.item()] + \" \"\n",
    "    return res\n",
    "\n",
    "seq_to_string(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ევალუციისთვის ვიყენებთ მარკეტერის სტატიებს. რომელიც სპეციალურად გაპარსა საბა სტურუამ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 30\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "წავიკითხოთ ტესტ-დატა"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/marketer-articles/articles.csv')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_list = list(test_data['text'])\n",
    "flattened_test_data = []\n",
    "for text in data_list:\n",
    "    if isinstance(text, str):\n",
    "        sent_text = nltk.sent_tokenize(text)\n",
    "        flattened_test_data += sent_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence = []\n",
    "for sent in flattened_test_data:\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        test_sequence.append(torch.tensor(word_to_ind.get(word, 0)))\n",
    "test_sequence = torch.tensor(test_sequence)\n",
    "test_data = batchify(test_sequence, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "evaluation_result = evaluate(model, test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
