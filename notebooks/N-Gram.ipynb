{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "from nltk import ngrams\n",
    "from nltk import FreqDist\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import glob, re\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "სტანდარტულად წავიკითხოთ მონაცემები. იქიდან გამომდინარე რომ ქეგლზე პრობლემები შეგვექმნა მეხსიერების სიმცირის გამო, ვიყენებთ მონაცემთა დაახლოებით **მესამედს**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for ind, filename in enumerate(glob.glob(\"../data/whole-data/*\")):\n",
    "    if ind % 10000 == 0:\n",
    "        print(ind, 'docs read')\n",
    "    if ind == 100000:\n",
    "        break\n",
    "    fd = open(filename, 'r')\n",
    "    text = fd.read() \n",
    "    docs.append(text)\n",
    "    fd.close()\n",
    "    \n",
    "    \n",
    "corpus = []\n",
    "for doc in docs:\n",
    "    doc = re.sub('([\"-.,!?()])', r' \\1 ', doc)\n",
    "    tokens = doc.split()\n",
    "    corpus.append(tokens)\n",
    "\n",
    "corpus = [doc for doc in corpus if len(doc) > 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "დავბეჭდოთ წაკითხული ტოკენები"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus), corpus[:10]\n",
    "random.shuffle(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = corpus[:7000], corpus[7000:]\n",
    "train[:10], test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ავაგოთ N-Gramის მოდელი. N-gram იყენებს სტატისტიკურ მონაცემებს ტექსტიდან და ცდილობს სიტყვების მიმდევრობათა სიხშირეების მიხედვით მომდევნო სიტყვის გამოცნობას."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tokens, n):\n",
    "    result = []\n",
    "    for sent in tokens:\n",
    "        sent = (n-1)*['<s>'] + sent + ['</s>']\n",
    "        result += [sent]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "დავა-flatten-ოთ მიღებული ტოკენები და ჩავამატოთ  დასაწყისის და დასასრულის აღმნიშვნელი ტოკენები"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(train, N)\n",
    "flattened_tokens = [item for sublist in train_data for item in sublist]\n",
    "\n",
    "test_data = preprocess(test, N)\n",
    "test_flattened = [item for sublist in test_data for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(flattened_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = \"<UNK>\"\n",
    "\n",
    "\n",
    "class NGram():\n",
    "    \"\"\"ნ-გრამის მოდელი მოცემული კორპუსისთვის\n",
    "\n",
    "        აკეთებს ტექსტის პრეპროცესირებას, სიხშირეების დათვლას და ალბათობების ნორმალიზაციას.\n",
    "        Args\n",
    "            train_data (list of str): წინადედებების სია.\n",
    "            n (int): ნ-გრამის n\n",
    "            laplace (int): lambda multiplier to use for laplace smoothing (default 1 for add-1 smoothing).\n",
    "    \"\"\"\n",
    "    def __init__(self, n, tokens, laplace=1):\n",
    "        self.n = n\n",
    "        self.laplace = laplace\n",
    "        self.tokens = tokens \n",
    "        self.vocab = FreqDist(self.tokens)\n",
    "        self.tokens = [token if self.vocab[token] > 1 else UNK for token in tokens]\n",
    "        self.model = self._create_model()\n",
    "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
    "\n",
    "#         self.model = defaultdict(lambda: defaultdict(lambda: 1))\n",
    "#         print(self.vocab)\n",
    "\n",
    "    def _create_model(self):\n",
    "        \"\"\"\n",
    "        ქმნის n-გრამ მოდელს მოცემული ტოკენებისთვის.\n",
    "        m = n-1. იყენებს nltk-ის ngram ბიბლიოთეკას\n",
    "        \n",
    "        აბრუნებს ნ-gram - შეწონილი ალბათობების წყვილების dictionary-ს\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.vocab)\n",
    "\n",
    "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
    "        n_vocab = nltk.FreqDist(n_grams)\n",
    "\n",
    "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
    "        m_vocab = nltk.FreqDist(m_grams)\n",
    "\n",
    "        def smoothed_count(n_gram, n_count):\n",
    "            m_gram = n_gram[:-1]\n",
    "            m_count = m_vocab[m_gram]\n",
    "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
    "\n",
    "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
    "        \n",
    "\n",
    "        \n",
    "    def _convert_oov(self, ngram):\n",
    "        \"\"\"\n",
    "        ანაცვებს მოდელისთვის უცნობ ნ-გრამს მისთვის ნაცნობი ნ-გრამით. \n",
    "        ამისავის UNK-ების მასკით თითოეულ წევრს ანაცვლებს UNK-ტოკენით.\n",
    "        ყველაზე ცუდ შემთხვევაში 2^ნ მცდელობა.\n",
    "        \"\"\"\n",
    "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "\n",
    "        ngram = (ngram,) if type(ngram) is str else ngram\n",
    "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
    "            if possible_known in self.model:\n",
    "                return possible_known\n",
    "\n",
    "            \n",
    "    def best_candidate(self, prev, i):\n",
    "        \"\"\"\n",
    "        აბრუნებს სავარაუდო გაგრძელებას ნგრამისა\n",
    "        \"\"\"\n",
    "        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev))\n",
    "        strs = [ngram for ngram, prob in candidates]\n",
    "        probs = [prob for ngram, prob in candidates]\n",
    "        if len(strs) == 0:\n",
    "            return (\"</s>\")\n",
    "        index = random.choices(range(len(probs)), probs)\n",
    "        return strs[index[0]]\n",
    "        \n",
    "    \n",
    "    def generate_sent(self, min_len=12, max_len=24):\n",
    "        sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n",
    "        while sent[-1] != \"</s>\":\n",
    "            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n",
    "            blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n",
    "            next_token = self.best_candidate(prev, 1)\n",
    "            sent.append(next_token)\n",
    "\n",
    "            if len(sent) >= max_len:\n",
    "                sent.append(\"</s>\")\n",
    "            \n",
    "        return ' '.join(sent)\n",
    "\n",
    "    \n",
    "    def perplexity(self, test_tokens):\n",
    "        \"\"\"\n",
    "        ითვლის მოდელის perpexity-ის ტესტ დატაზე\n",
    "        \n",
    "        \"\"\"\n",
    "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
    "        NN = len(test_tokens)\n",
    "\n",
    "        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n",
    "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
    "\n",
    "        return math.exp((-1/NN) * sum(map(math.log, probabilities)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = NGram(N, flattened_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(sent, n):\n",
    "    last_tokens = sent[-(n-1):]\n",
    "#     print(last_tokens)\n",
    "    next_word = n_gram.best_candidate(tuple(last_tokens), 1)\n",
    "    return sent+[next_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "დავწეროთ ფუნქცია, რომელიც აგრძელებს წინადადებას N ტოკენამდე"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_sent(sent, max_len=14):\n",
    "    for i in range (max_len):\n",
    "        sent = (predict_next_word(sent, N))\n",
    "        if sent[-1] == '</s>':\n",
    "            return sent\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "გავაგძელოთ მოცემული წინადადება:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_sent(['მსოფლიო','საეკლესიო','კრების', ',', 'ეკლესიათა'], 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ფუნქცია რომელიც აგენერირებს შემთხვევით წინადადებას"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram.generate_sent(max_len = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "მოდელის perplexity-ის დასათვლელად ვაგენერირებთ შემთხვევით წინადადებებს ტესტური მონაცემებიდან და ვამოწმებთ თუ სწორად აგრძელებს ჩვენი მოდელი. დამატებითი ინფორმაია იხილეთ რეპორტში"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram.perplexity(test_flattened)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
