{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from nltk.corpus import reuters\nfrom nltk import ngrams\nfrom nltk import FreqDist\nfrom collections import Counter, defaultdict\nimport nltk\nimport re\nimport json\nimport pickle\nimport numpy as np\nimport math\nimport random\nimport glob, re\nfrom itertools import product\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"სტანდარტულად წავიკითხოთ მონაცემები. იქიდან გამომდინარე რომ ქეგლზე პრობლემები შეგვექმნა მეხსიერების სიმცირის გამო, ვიყენებთ მონაცემთა დაახლოებით **მესამედს**"},{"metadata":{"trusted":true},"cell_type":"code","source":"docs = []\nfor ind, filename in enumerate(glob.glob(\"../data/whole-data/*\")):\n    if ind % 10000 == 0:\n        print(ind, 'docs read')\n    if ind == 100000:\n        break\n    fd = open(filename, 'r')\n    text = fd.read() \n    docs.append(text)\n    fd.close()\n    \n    \ncorpus = []\nfor doc in docs:\n    doc = re.sub('([\"-.,!?()])', r' \\1 ', doc)\n    tokens = doc.split()\n    corpus.append(tokens)\n\ncorpus = [doc for doc in corpus if len(doc) > 25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"დავბეჭდოთ წაკითხული ტოკენები"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(corpus), corpus[:10]\nrandom.shuffle(corpus)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = corpus[:7000], corpus[7000:]\ntrain[:10], test[:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ავაგოთ N-Gramის მოდელი. N-gram იყენებს სტატისტიკურ მონაცემებს ტექსტიდან და ცდილობს სიტყვების მიმდევრობათა სიხშირეების მიხედვით მომდევნო სიტყვის გამოცნობას."},{"metadata":{"trusted":true},"cell_type":"code","source":"N = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess(tokens, n):\n    result = []\n    for sent in tokens:\n        sent = (n-1)*['<s>'] + sent + ['</s>']\n        result += [sent]\n    return result\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"დავა-flatten-ოთ მიღებული ტოკენები და ჩავამატოთ  დასაწყისის და დასასრულის აღმნიშვნელი ტოკენები"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = preprocess(train, N)\nflattened_tokens = [item for sublist in train_data for item in sublist]\n\ntest_data = preprocess(test, N)\ntest_flattened = [item for sublist in test_data for item in sublist]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(flattened_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"UNK = \"<UNK>\"\n\n\nclass NGram():\n    \"\"\"ნ-გრამის მოდელი მოცემული კორპუსისთვის\n\n        აკეთებს ტექსტის პრეპროცესირებას, სიხშირეების დათვლას და ალბათობების ნორმალიზაციას.\n        Args\n            train_data (list of str): წინადედებების სია.\n            n (int): ნ-გრამის n\n            laplace (int): lambda multiplier to use for laplace smoothing (default 1 for add-1 smoothing).\n    \"\"\"\n    def __init__(self, n, tokens, laplace=1):\n        self.n = n\n        self.laplace = laplace\n        self.tokens = tokens \n        self.vocab = FreqDist(self.tokens)\n        self.tokens = [token if self.vocab[token] > 1 else UNK for token in tokens]\n        self.model = self._create_model()\n        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n\n#         self.model = defaultdict(lambda: defaultdict(lambda: 1))\n#         print(self.vocab)\n\n    def _create_model(self):\n        \"\"\"\n        ქმნის n-გრამ მოდელს მოცემული ტოკენებისთვის.\n        m = n-1. იყენებს nltk-ის ngram ბიბლიოთეკას\n        \n        აბრუნებს ნ-gram - შეწონილი ალბათობების წყვილების dictionary-ს\n        \"\"\"\n        vocab_size = len(self.vocab)\n\n        n_grams = nltk.ngrams(self.tokens, self.n)\n        n_vocab = nltk.FreqDist(n_grams)\n\n        m_grams = nltk.ngrams(self.tokens, self.n-1)\n        m_vocab = nltk.FreqDist(m_grams)\n\n        def smoothed_count(n_gram, n_count):\n            m_gram = n_gram[:-1]\n            m_count = m_vocab[m_gram]\n            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n\n        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n        \n\n        \n    def _convert_oov(self, ngram):\n        \"\"\"\n        ანაცვებს მოდელისთვის უცნობ ნ-გრამს მისთვის ნაცნობი ნ-გრამით. \n        ამისავის UNK-ების მასკით თითოეულ წევრს ანაცვლებს UNK-ტოკენით.\n        ყველაზე ცუდ შემთხვევაში 2^ნ მცდელობა.\n        \"\"\"\n        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n\n        ngram = (ngram,) if type(ngram) is str else ngram\n        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n            if possible_known in self.model:\n                return possible_known\n\n            \n    def best_candidate(self, prev, i):\n        \"\"\"\n        აბრუნებს სავარაუდო გაგრძელებას ნგრამისა\n        \"\"\"\n        candidates = list(((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev))\n        strs = [ngram for ngram, prob in candidates]\n        probs = [prob for ngram, prob in candidates]\n        if len(strs) == 0:\n            return (\"</s>\")\n        index = random.choices(range(len(probs)), probs)\n        return strs[index[0]]\n        \n    \n    def generate_sent(self, min_len=12, max_len=24):\n        sent, prob = [\"<s>\"] * max(1, self.n-1), 1\n        while sent[-1] != \"</s>\":\n            prev = () if self.n == 1 else tuple(sent[-(self.n-1):])\n            blacklist = sent + ([\"</s>\"] if len(sent) < min_len else [])\n            next_token = self.best_candidate(prev, 1)\n            sent.append(next_token)\n\n            if len(sent) >= max_len:\n                sent.append(\"</s>\")\n            \n        return ' '.join(sent)\n\n    \n    def perplexity(self, test_tokens):\n        \"\"\"\n        ითვლის მოდელის perpexity-ის ტესტ დატაზე\n        \n        \"\"\"\n        test_ngrams = nltk.ngrams(test_tokens, self.n)\n        NN = len(test_tokens)\n\n        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n        probabilities = [self.model[ngram] for ngram in known_ngrams]\n\n        return math.exp((-1/NN) * sum(map(math.log, probabilities)))\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_gram = NGram(N, flattened_tokens)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict_next_word(sent, n):\n    last_tokens = sent[-(n-1):]\n#     print(last_tokens)\n    next_word = n_gram.best_candidate(tuple(last_tokens), 1)\n    return sent+[next_word]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"დავწეროთ ფუნქცია, რომელიც აგრძელებს წინადადებას N ტოკენამდე"},{"metadata":{"trusted":true},"cell_type":"code","source":"def continue_sent(sent, max_len=14):\n    for i in range (max_len):\n        sent = (predict_next_word(sent, N))\n        if sent[-1] == '</s>':\n            return sent\n    return sent","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"გავაგძელოთ მოცემული წინადადება:"},{"metadata":{"trusted":true},"cell_type":"code","source":"continue_sent(['მსოფლიო','საეკლესიო','კრების', ',', 'ეკლესიათა'], 6)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"ფუნქცია რომელიც აგენერირებს შემთხვევით წინადადებას"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_gram.generate_sent(max_len = 50)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"მოდელის perplexity-ის დასათვლელად ვაგენერირებთ შემთხვევით წინადადებებს ტესტური მონაცემებიდან და ვამოწმებთ თუ სწორად აგრძელებს ჩვენი მოდელი. დამატებითი ინფორმაია იხილეთ რეპორტში"},{"metadata":{"trusted":true},"cell_type":"code","source":"n_gram.perplexity(test_flattened)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}