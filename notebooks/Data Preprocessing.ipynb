{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-20T16:24:51.508293Z",
     "iopub.status.busy": "2021-01-20T16:24:51.507506Z",
     "iopub.status.idle": "2021-01-20T16:24:53.414357Z",
     "shell.execute_reply": "2021-01-20T16:24:53.414922Z"
    },
    "papermill": {
     "duration": 1.927134,
     "end_time": "2021-01-20T16:24:53.415102",
     "exception": false,
     "start_time": "2021-01-20T16:24:51.487968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import glob, re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize as tokenize\n",
    "import string, random, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:24:53.447275Z",
     "iopub.status.busy": "2021-01-20T16:24:53.446243Z",
     "iopub.status.idle": "2021-01-20T16:54:03.482849Z",
     "shell.execute_reply": "2021-01-20T16:54:03.482241Z"
    },
    "papermill": {
     "duration": 1750.056096,
     "end_time": "2021-01-20T16:54:03.482982",
     "exception": false,
     "start_time": "2021-01-20T16:24:53.426886",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs read\n",
      "10000 docs read\n",
      "20000 docs read\n",
      "30000 docs read\n",
      "40000 docs read\n",
      "50000 docs read\n",
      "60000 docs read\n",
      "70000 docs read\n",
      "80000 docs read\n",
      "90000 docs read\n",
      "100000 docs read\n",
      "110000 docs read\n",
      "120000 docs read\n",
      "130000 docs read\n",
      "140000 docs read\n",
      "150000 docs read\n",
      "160000 docs read\n",
      "170000 docs read\n",
      "180000 docs read\n",
      "190000 docs read\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "for ind, filename in enumerate(glob.glob(\"../data/whole-data/*\")):\n",
    "    if ind % 10000 == 0:\n",
    "        print(ind, 'docs read')\n",
    "    fd = open(filename, 'r')\n",
    "    text = fd.read() \n",
    "    docs.append(text)\n",
    "    fd.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ვიკიპედიას კორპუსში ბევრი არასასურველი სიტყვაა და ჯობს მინიმალურამდე დავიყვანოთ მათი რაოდენობა. ამისთვის, თითოეულ სტატიაში ვითვლით რამდენი \"კარგი\" და \"ცუდი\" სიტყვაა და მათი სხვაობის მიხედვით ტოპ 50 000 სტატიას ვარჩევთ. ამ მიდგომით შეგვრჩება 50 000 სტატია, რომელშიც ყველაზე მეტი სასურველი სიტყვაა და თან თავიდან ავირიდებთ მცირე ზომის სტატიებს. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:54:03.547114Z",
     "iopub.status.busy": "2021-01-20T16:54:03.536158Z",
     "iopub.status.idle": "2021-01-20T16:54:23.835510Z",
     "shell.execute_reply": "2021-01-20T16:54:23.834941Z"
    },
    "papermill": {
     "duration": 20.335656,
     "end_time": "2021-01-20T16:54:23.835731",
     "exception": false,
     "start_time": "2021-01-20T16:54:03.500075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized = [tokenize(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:54:23.876600Z",
     "iopub.status.busy": "2021-01-20T16:54:23.875421Z",
     "iopub.status.idle": "2021-01-20T16:54:23.878681Z",
     "shell.execute_reply": "2021-01-20T16:54:23.878018Z"
    },
    "papermill": {
     "duration": 0.025741,
     "end_time": "2021-01-20T16:54:23.878791",
     "exception": false,
     "start_time": "2021-01-20T16:54:23.853050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# თუ სიტყვა შეიცავს ერთ სიმბოლოს მაინც, რომელიც ქვედა სიმბოლოების სიაში არ შედის, ესე იგი არასასურველი სიტყვაა.\n",
    "punct = '!\"#%&+,-./:;=?()²' + \"'\"\n",
    "letters = 'აბგდევზთიკლმნოპჟრსტუფქღყშჩცძწჭხჯჰ'\n",
    "numbers = '0123456789'\n",
    "\n",
    "filt = list(punct + letters + numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:54:23.924375Z",
     "iopub.status.busy": "2021-01-20T16:54:23.921764Z",
     "iopub.status.idle": "2021-01-20T16:57:33.636757Z",
     "shell.execute_reply": "2021-01-20T16:57:33.635865Z"
    },
    "papermill": {
     "duration": 189.740861,
     "end_time": "2021-01-20T16:57:33.636885",
     "exception": false,
     "start_time": "2021-01-20T16:54:23.896024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs processed\n",
      "10000 docs processed\n",
      "20000 docs processed\n",
      "30000 docs processed\n",
      "40000 docs processed\n",
      "50000 docs processed\n",
      "60000 docs processed\n",
      "70000 docs processed\n",
      "80000 docs processed\n",
      "90000 docs processed\n",
      "100000 docs processed\n",
      "110000 docs processed\n",
      "120000 docs processed\n",
      "130000 docs processed\n",
      "140000 docs processed\n",
      "150000 docs processed\n",
      "160000 docs processed\n",
      "170000 docs processed\n",
      "180000 docs processed\n",
      "190000 docs processed\n"
     ]
    }
   ],
   "source": [
    "doc_stats = []\n",
    "bad_words = Counter()\n",
    "good_words = Counter()\n",
    "for ind, doc in enumerate(tokenized):\n",
    "    if ind % 10000 == 0:\n",
    "        print(ind, 'docs processed')\n",
    "    size = len(doc)\n",
    "    bad_n = 0\n",
    "    good_n = 0\n",
    "    for word in doc:\n",
    "        bad = False\n",
    "        for symb in word:\n",
    "            if symb not in filt:\n",
    "                bad = True\n",
    "                bad_words.update([word])\n",
    "                bad_n += 1\n",
    "                break\n",
    "        if not bad:\n",
    "            good_words.update([word])\n",
    "            good_n += 1\n",
    "    doc_stats.append((good_n, bad_n, ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:57:33.687395Z",
     "iopub.status.busy": "2021-01-20T16:57:33.686768Z",
     "iopub.status.idle": "2021-01-20T16:57:33.749191Z",
     "shell.execute_reply": "2021-01-20T16:57:33.749742Z"
    },
    "papermill": {
     "duration": 0.089326,
     "end_time": "2021-01-20T16:57:33.749897",
     "exception": false,
     "start_time": "2021-01-20T16:57:33.660571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# დავასორტიროთ კარგ სიტყვებს - ცუდი სიტყვების მიხედვით\n",
    "results = [(stat[0]-stat[1], stat[2]) for stat in doc_stats]\n",
    "results.sort(key=lambda tup:tup[0], reverse=True)\n",
    "results = results[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:57:34.013722Z",
     "iopub.status.busy": "2021-01-20T16:57:34.013025Z",
     "iopub.status.idle": "2021-01-20T16:57:34.038897Z",
     "shell.execute_reply": "2021-01-20T16:57:34.039415Z"
    },
    "papermill": {
     "duration": 0.05305,
     "end_time": "2021-01-20T16:57:34.039553",
     "exception": false,
     "start_time": "2021-01-20T16:57:33.986503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = [ind for score, ind in results]\n",
    "articles = [docs[ind] for ind in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-20T16:57:34.090501Z",
     "iopub.status.busy": "2021-01-20T16:57:34.089836Z",
     "iopub.status.idle": "2021-01-20T16:57:34.166185Z",
     "shell.execute_reply": "2021-01-20T16:57:34.165497Z"
    },
    "papermill": {
     "duration": 0.102916,
     "end_time": "2021-01-20T16:57:34.166300",
     "exception": false,
     "start_time": "2021-01-20T16:57:34.063384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ავრიოთ და გავყოთ ორ ნაწილად\n",
    "random.shuffle(articles)\n",
    "train = articles[:45000]\n",
    "validation = articles[45000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ფუნქციები იმისთვის გვაქვს, რომ ტოკენიზატორი ვერ ითვალისწინებს რამდენიმე შემთხვევას\n",
    "filtered_train = []\n",
    "for text in train:\n",
    "    text = text.replace('\"', ' \" ')\n",
    "    text = text.replace('”', ' \" ')\n",
    "    text = text.replace('“', ' \" ')\n",
    "    tokenized = tokenize(text)\n",
    "    filtered_train.append(tokenized)\n",
    "\n",
    "filtered_validation = []\n",
    "for text in validation:\n",
    "    text = text.replace('\"', ' \" ')\n",
    "    text = text.replace('”', ' \" ')\n",
    "    text = text.replace('“', ' \" ')\n",
    "    tokenized = tokenize(text)\n",
    "    filtered_validation.append(tokenized)\n",
    "\n",
    "train = filtered_train\n",
    "validation = filtered_validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "საქმე იმაშია, რომ გვაქვს ძალიან ბევრი უნიკალური ტოკენი და უმეტესობის შეხვედრის სიხშირე არის საკმაოდ ცოტა.\n",
    "გვიწევს ყველაზე ხშირი ტოკენების გადანახვა და დანარჩენების unk ტოკენებით შეცვლა. \n",
    "პრობლემა იმაშია, რომ ამ მიდგომამ არ გაამართლა. იმდენად ბევრი unk ტოკენი ხვდებოდა შემდგომ მოდელს, რომ პრიორიტეტს სწორედ ამ ტოკენს ანიჭებდა.\n",
    "ამის საწინააღმდეგოდ, თუ vocab-ში ბევრ სიტყვას შევიყვანთ და ამით შევამცირებთ unk ტოკენების რაოდენობას, \n",
    "მაშინ მოდელი კომპლექსური გახდება და ამავდროულად overfitting-ზე გავა, რადგან მონაცემები ცოტაა და სიტყვების დიდი ნაწილი სულ 5-10ჯერ იქნება ალბათ კორპუსში.\n",
    "\n",
    "\n",
    "იმისთვის, რომ მდგომარეობა შეგვემსუბუქებინა, გადავწყვიტეთ შემოგვეღო ბევრნაირი unk ტოკენი. აღვწერ თითოეულს:\n",
    "\n",
    "NUM_SML - ციფრები\n",
    "\n",
    "NUM_MED - რიცხვები 10დან 2100-მდე (კორპუსიდან გამომდინარე, ძირითადად იტორიული წლები შედის აქ)\n",
    "\n",
    "NUM_LRG - 2100დან ზემოთ\n",
    "\n",
    "SYMB_UNK - სიმბოლო, რომელიც არ შედის ზემოთ შედგენილ სიმბოლოების სიაში\n",
    "\n",
    "FOREIGN_UNK - უცხოური სიტყვა\n",
    "\n",
    "GEO_UNK - ქართული სიტყვა, რომელიც VOCAB-ში არაა\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    filtered = []\n",
    "    for token in tokens:\n",
    "        if len(token) == 1:\n",
    "            if token in numbers:\n",
    "                filtered.append('NUM_SML')\n",
    "            elif token not in punct and token not in letters:\n",
    "                filtered.append('SYMB_UNK')\n",
    "            else:\n",
    "                filtered.append(token)\n",
    "        elif token.isdigit():\n",
    "            try:\n",
    "                num = float(token)\n",
    "                if num < 2100:\n",
    "                    filtered.append('NUM_MED')\n",
    "                else:\n",
    "                    filtered.append('NUM_LRG')\n",
    "            except:\n",
    "                filtered.append('NUM_MED')\n",
    "        else:\n",
    "            foreign = False\n",
    "            for sm in token:\n",
    "                if sm not in letters:\n",
    "                    foreign = True\n",
    "                    break\n",
    "            if foreign:\n",
    "                filtered.append('FOREIGN_UNK')\n",
    "            else:\n",
    "                filtered.append(token)\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "validation = [filter_tokens(doc) for doc in validation]\n",
    "train = [filter_tokens(doc) for doc in train]\n",
    "for doc in train:\n",
    "    cnt.update(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ['GEO_UNK'] + [word for word, freq in cnt.most_common()[:90000]]\n",
    "vocab = sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding unk words to train\n",
      "adding unk words to validation\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "vocab_set = set(vocab)\n",
    "def add_geo_unk_words(tokens):\n",
    "    filtered = []\n",
    "    for word in tokens:\n",
    "        if word in vocab_set:\n",
    "            filtered.append(word)\n",
    "        else:\n",
    "            filtered.append('GEO_UNK')\n",
    "    return filtered\n",
    "\n",
    "print('adding unk words to train')\n",
    "train = [add_geo_unk_words(doc) for doc in train]\n",
    "print('adding unk words to validation')\n",
    "validation = [add_geo_unk_words(doc) for doc in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "if not os.path.exists('../data/vocab'):\n",
    "    os.makedirs('../data/vocab')\n",
    "    \n",
    "with open('../data/vocab/vocab.json', 'w') as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False)\n",
    "    \n",
    "if not os.path.exists('../data/train'):\n",
    "    os.makedirs('../data/train')\n",
    "    \n",
    "with open('../data/train/train.json', 'w') as f:\n",
    "    json.dump(train, f, ensure_ascii=False)\n",
    "    \n",
    "if not os.path.exists('../data/validation'):\n",
    "    os.makedirs('../data/validation')\n",
    "\n",
    "with open('../data/validation/validation.json', 'w') as f:\n",
    "    json.dump(validation, f, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 1971.803932,
   "end_time": "2021-01-20T16:57:38.218642",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-20T16:24:46.414710",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
