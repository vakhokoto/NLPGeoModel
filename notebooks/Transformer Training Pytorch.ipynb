{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-23T20:47:30.283482Z",
     "iopub.status.busy": "2021-01-23T20:47:30.282932Z",
     "iopub.status.idle": "2021-01-23T20:47:33.130299Z",
     "shell.execute_reply": "2021-01-23T20:47:33.129079Z"
    },
    "papermill": {
     "duration": 2.865473,
     "end_time": "2021-01-23T20:47:33.130415",
     "exception": false,
     "start_time": "2021-01-23T20:47:30.264942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, json\n",
    "from collections import Counter\n",
    "from nltk.tokenize import wordpunct_tokenize as tokenize\n",
    "import io, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from gensim.models import word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:47:33.160329Z",
     "iopub.status.busy": "2021-01-23T20:47:33.159689Z",
     "iopub.status.idle": "2021-01-23T20:47:41.353942Z",
     "shell.execute_reply": "2021-01-23T20:47:41.352804Z"
    },
    "papermill": {
     "duration": 8.212771,
     "end_time": "2021-01-23T20:47:41.354065",
     "exception": false,
     "start_time": "2021-01-23T20:47:33.141294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../data/train/train.json', 'r') as f:\n",
    "    train = json.load(f)\n",
    "    \n",
    "with open('../data/validation/validation.json', 'r') as f:\n",
    "    validation = json.load(f)\n",
    "    \n",
    "with open('../data/vocab/vocab.json', 'r') as f:\n",
    "    vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:47:41.415011Z",
     "iopub.status.busy": "2021-01-23T20:47:41.393781Z",
     "iopub.status.idle": "2021-01-23T20:47:41.417084Z",
     "shell.execute_reply": "2021-01-23T20:47:41.417466Z"
    },
    "papermill": {
     "duration": 0.053013,
     "end_time": "2021-01-23T20:47:41.417583",
     "exception": false,
     "start_time": "2021-01-23T20:47:41.364570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_to_ind = {word:ind for ind, word in enumerate(vocab)}\n",
    "ind_to_word = {ind:word for ind, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009929,
     "end_time": "2021-01-23T20:47:41.437924",
     "exception": false,
     "start_time": "2021-01-23T20:47:41.427995",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "მოდელისთვის გვჭირდება, რომ მთელი კორპუსი ერთ დიდ მიმდევრობად წარმოვადგინოთ. ერთ ლისტში არ გვაკეთებინებდა და ორად დავყავით. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:47:41.469182Z",
     "iopub.status.busy": "2021-01-23T20:47:41.468649Z",
     "iopub.status.idle": "2021-01-23T20:50:29.446072Z",
     "shell.execute_reply": "2021-01-23T20:50:29.445413Z"
    },
    "papermill": {
     "duration": 167.998178,
     "end_time": "2021-01-23T20:50:29.446195",
     "exception": false,
     "start_time": "2021-01-23T20:47:41.448017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_sequence1 = []\n",
    "for ind, doc in enumerate(train):\n",
    "    if ind < 25000:\n",
    "        for word in doc:\n",
    "            train_sequence1.append(torch.tensor(word_to_ind.get(word, 0)))\n",
    "\n",
    "train_sequence1 = torch.tensor(train_sequence1)\n",
    "\n",
    "train_sequence2 = []\n",
    "for ind, doc in enumerate(train):\n",
    "    if ind >= 25000:\n",
    "        for word in doc:\n",
    "            train_sequence2.append(torch.tensor(word_to_ind.get(word, 0)))\n",
    "\n",
    "train_sequence2 = torch.tensor(train_sequence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:29.476401Z",
     "iopub.status.busy": "2021-01-23T20:50:29.473944Z",
     "iopub.status.idle": "2021-01-23T20:50:47.495871Z",
     "shell.execute_reply": "2021-01-23T20:50:47.495324Z"
    },
    "papermill": {
     "duration": 18.038587,
     "end_time": "2021-01-23T20:50:47.495982",
     "exception": false,
     "start_time": "2021-01-23T20:50:29.457395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_sequence = []\n",
    "for doc in validation:\n",
    "    for word in doc:\n",
    "        valid_sequence.append(torch.tensor(word_to_ind.get(word, 0)))\n",
    "\n",
    "valid_sequence = torch.tensor(valid_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:47.520785Z",
     "iopub.status.busy": "2021-01-23T20:50:47.519973Z",
     "iopub.status.idle": "2021-01-23T20:50:47.892068Z",
     "shell.execute_reply": "2021-01-23T20:50:47.891540Z"
    },
    "papermill": {
     "duration": 0.384967,
     "end_time": "2021-01-23T20:50:47.892195",
     "exception": false,
     "start_time": "2021-01-23T20:50:47.507228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:47.925226Z",
     "iopub.status.busy": "2021-01-23T20:50:47.924561Z",
     "iopub.status.idle": "2021-01-23T20:50:51.598082Z",
     "shell.execute_reply": "2021-01-23T20:50:51.597577Z"
    },
    "papermill": {
     "duration": 3.694392,
     "end_time": "2021-01-23T20:50:51.598193",
     "exception": false,
     "start_time": "2021-01-23T20:50:47.903801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ბეჩებად დაყოფა\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data1 = batchify(train_sequence1, batch_size)\n",
    "train_data2 = batchify(train_sequence2, batch_size)\n",
    "val_data = batchify(valid_sequence, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:51.627341Z",
     "iopub.status.busy": "2021-01-23T20:50:51.625596Z",
     "iopub.status.idle": "2021-01-23T20:50:51.627930Z",
     "shell.execute_reply": "2021-01-23T20:50:51.628328Z"
    },
    "papermill": {
     "duration": 0.019049,
     "end_time": "2021-01-23T20:50:51.628425",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.609376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# რა სიგრძის მიმდევრობები უნდა გადავცეთ მოდელს\n",
    "bptt = 30\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010863,
     "end_time": "2021-01-23T20:50:51.650108",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.639245",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "მოგეხსენებათ attention მექანიზმი არ ითვალისწინებს ტოკენების მიმდევრობას. ეს კი პრობლემაა, რადგან როცა საქმე ტექსტს ეხება, სიტყვათა მიმდევრობა ძალიან მნიშვნელოვანია.\n",
    "\n",
    "მოცემული პრობლემის მოგვარება შეგვიძლია პოზიციური ენკოდირებით. იდეა იმაშია, რომ ავაგოთ სხვადასხვა სიხშირის სინუსოიდა\n",
    "\n",
    "![positional encoding](https://image.slidesharecdn.com/paper-review-attention-180607163404/95/attention-is-all-you-need-upc-reading-group-2018-by-santi-pascual-34-638.jpg?cb=1528389383)\n",
    "\n",
    "მნიშვნელოვანია, რომ იყოს იმავე განზომილების, რაც ჩვენი ვორდ ემბედინგებია, რათა საბოლოოდ შევძლოთ მათი მიმატება.\n",
    "\n",
    "ბევრნაირი სინუსოიდას საშუალებით, პოზიციურ ენკოდირებას შეუძლია რელაციური პოზიციის ინფორმაციის ჩადება თითოეულ ემბედინგში და ასე ვაგვარებთ ზემოხსენებულ პრობლემას."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:51.683241Z",
     "iopub.status.busy": "2021-01-23T20:50:51.681628Z",
     "iopub.status.idle": "2021-01-23T20:50:51.684232Z",
     "shell.execute_reply": "2021-01-23T20:50:51.684673Z"
    },
    "papermill": {
     "duration": 0.024013,
     "end_time": "2021-01-23T20:50:51.684795",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.660782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        სინუსოიდების აგება თავიდან ხდება ერთხელ და შემდეგ გამოთვლილ მნიშვნელობებს ვიყენებთ,\n",
    "        რათა ემბედინგებს დავუმატოთ attention-მდე.\n",
    "        აღსანიშნავია, რომ pe, ანუ ჩვენი სინუსოიდების კომბინაცია, არ არის მოდელის პარამეტრი და \n",
    "        სწორედ ამიტომაა დამატებული register_buffer მეთოდით, რათა მოდელს არ შეშლოდა პარამეტრში, \n",
    "        მაგრამ მაინც სთეითის ნაწილი ყოფილიყო. \n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01098,
     "end_time": "2021-01-23T20:50:51.706511",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.695531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "რაც შეეხება მოდელს, ვიყენებთ ტრანსფორმერის ორიგინალ არქიტექტურას, რომელიც ნაშრომში - \"attention is all you need\" იქნა შემოთავაზებული. \n",
    "\n",
    "![transformer architecture](https://miro.medium.com/max/856/1*ZCFSvkKtppgew3cc7BIaug.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:51.743474Z",
     "iopub.status.busy": "2021-01-23T20:50:51.742915Z",
     "iopub.status.idle": "2021-01-23T20:50:51.746240Z",
     "shell.execute_reply": "2021-01-23T20:50:51.745815Z"
    },
    "papermill": {
     "duration": 0.026948,
     "end_time": "2021-01-23T20:50:51.746318",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.719370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5, weights=None):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        if weights is None:\n",
    "            self.encoder = nn.Embedding(ntoken, ninp)        \n",
    "        else:\n",
    "            self.encoder = nn.Embedding.from_pretrained(weights)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:51.775369Z",
     "iopub.status.busy": "2021-01-23T20:50:51.774762Z",
     "iopub.status.idle": "2021-01-23T20:50:55.790633Z",
     "shell.execute_reply": "2021-01-23T20:50:55.790121Z"
    },
    "papermill": {
     "duration": 4.033346,
     "end_time": "2021-01-23T20:50:55.790754",
     "exception": false,
     "start_time": "2021-01-23T20:50:51.757408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pretrained_weights(model_path):\n",
    "    w2v = word2vec.Word2Vec.load(model_path)\n",
    "    embs = []\n",
    "    for word in vocab:\n",
    "        if word in w2v.wv.vocab:\n",
    "            embs.append(w2v.wv[word])\n",
    "        else:\n",
    "            embs.append(np.random.uniform(low=-0.5, high=0.5, size=(w2v.wv.vectors.shape[1],)))\n",
    "    embs = np.array(embs)\n",
    "    weights = torch.LongTensor(embs).float()\n",
    "    return weights\n",
    "\n",
    "w2v_path = '../input/nlp-word2vec/w2vemb100wind4.model'\n",
    "weights = get_pretrained_weights(w2v_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:55.819982Z",
     "iopub.status.busy": "2021-01-23T20:50:55.819344Z",
     "iopub.status.idle": "2021-01-23T20:50:56.141118Z",
     "shell.execute_reply": "2021-01-23T20:50:56.140639Z"
    },
    "papermill": {
     "duration": 0.337673,
     "end_time": "2021-01-23T20:50:56.141309",
     "exception": false,
     "start_time": "2021-01-23T20:50:55.803636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ntokens = len(word_to_ind) # vocab-ის ზომა\n",
    "emsize = 100 # ემბედინგის განზომილება\n",
    "nhid = 200 # ენკოდერში ფიდფორვარდ ნეთვორქის ჰიდენ სთეითის განზომილება\n",
    "nlayers = 2 # ენკოდერ ლეიერების რაოდენობა\n",
    "nhead = 2 # attention-ების რაოდენობა\n",
    "dropout = 0.3 # დროფაუთი\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:56.184263Z",
     "iopub.status.busy": "2021-01-23T20:50:56.183616Z",
     "iopub.status.idle": "2021-01-23T20:50:56.621094Z",
     "shell.execute_reply": "2021-01-23T20:50:56.620563Z"
    },
    "papermill": {
     "duration": 0.46818,
     "end_time": "2021-01-23T20:50:56.621213",
     "exception": false,
     "start_time": "2021-01-23T20:50:56.153033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1.5\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(train_data):\n",
    "    # თრეინ მოუდი\n",
    "    model.train() \n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    # მასქინგი აუცილებელია, რათა დექოდერი ვერ ხედავდეს მომდვნო სიტყვებს\n",
    "    # კვადრატული კი იმიტომაა, რომ ყოველი ეტაპისთვის იყო დამასკული\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        # გრადიენტის ქლიფინგი\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 1000\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            # როცა ქროს ენთროფი ლოსს ვიყენებთ, პირდაპირ შეგვიძლია exp-ის მორტყმით\n",
    "            # გამოვთვალოთ ფერფლექსითი\n",
    "            perplexity = math.exp(cur_loss)\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, perplexity))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-23T20:50:56.653822Z",
     "iopub.status.busy": "2021-01-23T20:50:56.653244Z",
     "iopub.status.idle": "2021-01-24T00:05:00.196629Z",
     "shell.execute_reply": "2021-01-24T00:05:00.197275Z"
    },
    "papermill": {
     "duration": 11643.564512,
     "end_time": "2021-01-24T00:05:00.197426",
     "exception": false,
     "start_time": "2021-01-23T20:50:56.632914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:370: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  1000/18571 batches | lr 1.50 | ms/batch 22.68 | loss 13.62 | ppl 818765.65\n",
      "| epoch   1 |  2000/18571 batches | lr 1.50 | ms/batch 21.85 | loss  7.60 | ppl  1997.08\n",
      "| epoch   1 |  3000/18571 batches | lr 1.50 | ms/batch 22.14 | loss  7.58 | ppl  1950.00\n",
      "| epoch   1 |  4000/18571 batches | lr 1.50 | ms/batch 21.90 | loss  7.55 | ppl  1909.09\n",
      "| epoch   1 |  5000/18571 batches | lr 1.50 | ms/batch 22.05 | loss  7.48 | ppl  1771.51\n",
      "| epoch   1 |  6000/18571 batches | lr 1.50 | ms/batch 21.97 | loss  7.49 | ppl  1793.80\n",
      "| epoch   1 |  7000/18571 batches | lr 1.50 | ms/batch 21.84 | loss  7.44 | ppl  1702.05\n",
      "| epoch   1 |  8000/18571 batches | lr 1.50 | ms/batch 22.27 | loss  7.45 | ppl  1715.17\n",
      "| epoch   1 |  9000/18571 batches | lr 1.50 | ms/batch 21.79 | loss  7.45 | ppl  1717.83\n",
      "| epoch   1 | 10000/18571 batches | lr 1.50 | ms/batch 21.80 | loss  7.43 | ppl  1679.42\n",
      "| epoch   1 | 11000/18571 batches | lr 1.50 | ms/batch 22.29 | loss  7.43 | ppl  1688.24\n",
      "| epoch   1 | 12000/18571 batches | lr 1.50 | ms/batch 21.86 | loss  7.44 | ppl  1705.71\n",
      "| epoch   1 | 13000/18571 batches | lr 1.50 | ms/batch 21.71 | loss  7.40 | ppl  1631.67\n",
      "| epoch   1 | 14000/18571 batches | lr 1.50 | ms/batch 22.68 | loss  7.38 | ppl  1607.08\n",
      "| epoch   1 | 15000/18571 batches | lr 1.50 | ms/batch 22.05 | loss  7.41 | ppl  1658.26\n",
      "| epoch   1 | 16000/18571 batches | lr 1.50 | ms/batch 22.02 | loss  7.39 | ppl  1627.65\n",
      "| epoch   1 | 17000/18571 batches | lr 1.50 | ms/batch 22.10 | loss  7.39 | ppl  1612.48\n",
      "| epoch   1 | 18000/18571 batches | lr 1.50 | ms/batch 21.89 | loss  7.39 | ppl  1612.05\n",
      "| epoch   1 |  1000/15143 batches | lr 1.50 | ms/batch 22.21 | loss  7.43 | ppl  1688.81\n",
      "| epoch   1 |  2000/15143 batches | lr 1.50 | ms/batch 21.81 | loss  7.38 | ppl  1609.96\n",
      "| epoch   1 |  3000/15143 batches | lr 1.50 | ms/batch 22.13 | loss  7.40 | ppl  1628.51\n",
      "| epoch   1 |  4000/15143 batches | lr 1.50 | ms/batch 22.04 | loss  7.39 | ppl  1622.71\n",
      "| epoch   1 |  5000/15143 batches | lr 1.50 | ms/batch 21.79 | loss  7.32 | ppl  1512.00\n",
      "| epoch   1 |  6000/15143 batches | lr 1.50 | ms/batch 22.04 | loss  7.40 | ppl  1638.51\n",
      "| epoch   1 |  7000/15143 batches | lr 1.50 | ms/batch 21.88 | loss  7.38 | ppl  1607.73\n",
      "| epoch   1 |  8000/15143 batches | lr 1.50 | ms/batch 21.86 | loss  7.37 | ppl  1588.23\n",
      "| epoch   1 |  9000/15143 batches | lr 1.50 | ms/batch 22.14 | loss  7.36 | ppl  1577.96\n",
      "| epoch   1 | 10000/15143 batches | lr 1.50 | ms/batch 21.96 | loss  7.40 | ppl  1628.62\n",
      "| epoch   1 | 11000/15143 batches | lr 1.50 | ms/batch 22.20 | loss  7.39 | ppl  1612.31\n",
      "| epoch   1 | 12000/15143 batches | lr 1.50 | ms/batch 21.83 | loss  7.39 | ppl  1627.82\n",
      "| epoch   1 | 13000/15143 batches | lr 1.50 | ms/batch 21.86 | loss  7.39 | ppl  1620.12\n",
      "| epoch   1 | 14000/15143 batches | lr 1.50 | ms/batch 22.24 | loss  7.37 | ppl  1593.47\n",
      "| epoch   1 | 15000/15143 batches | lr 1.50 | ms/batch 21.74 | loss  7.40 | ppl  1636.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 775.31s | valid loss  7.50 | valid ppl  1817.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  1000/18571 batches | lr 1.35 | ms/batch 21.83 | loss  7.33 | ppl  1523.17\n",
      "| epoch   2 |  2000/18571 batches | lr 1.35 | ms/batch 21.88 | loss  7.26 | ppl  1421.70\n",
      "| epoch   2 |  3000/18571 batches | lr 1.35 | ms/batch 22.25 | loss  7.28 | ppl  1454.32\n",
      "| epoch   2 |  4000/18571 batches | lr 1.35 | ms/batch 21.68 | loss  7.29 | ppl  1461.69\n",
      "| epoch   2 |  5000/18571 batches | lr 1.35 | ms/batch 21.85 | loss  7.25 | ppl  1403.33\n",
      "| epoch   2 |  6000/18571 batches | lr 1.35 | ms/batch 22.41 | loss  7.26 | ppl  1424.80\n",
      "| epoch   2 |  7000/18571 batches | lr 1.35 | ms/batch 21.75 | loss  7.22 | ppl  1367.91\n",
      "| epoch   2 |  8000/18571 batches | lr 1.35 | ms/batch 22.07 | loss  7.24 | ppl  1397.49\n",
      "| epoch   2 |  9000/18571 batches | lr 1.35 | ms/batch 22.30 | loss  7.25 | ppl  1414.20\n",
      "| epoch   2 | 10000/18571 batches | lr 1.35 | ms/batch 21.77 | loss  7.24 | ppl  1398.52\n",
      "| epoch   2 | 11000/18571 batches | lr 1.35 | ms/batch 22.35 | loss  7.25 | ppl  1412.51\n",
      "| epoch   2 | 12000/18571 batches | lr 1.35 | ms/batch 21.95 | loss  7.27 | ppl  1438.60\n",
      "| epoch   2 | 13000/18571 batches | lr 1.35 | ms/batch 21.86 | loss  7.24 | ppl  1388.51\n",
      "| epoch   2 | 14000/18571 batches | lr 1.35 | ms/batch 22.05 | loss  7.23 | ppl  1378.75\n",
      "| epoch   2 | 15000/18571 batches | lr 1.35 | ms/batch 21.91 | loss  7.26 | ppl  1425.69\n",
      "| epoch   2 | 16000/18571 batches | lr 1.35 | ms/batch 21.86 | loss  7.24 | ppl  1400.10\n",
      "| epoch   2 | 17000/18571 batches | lr 1.35 | ms/batch 22.15 | loss  7.25 | ppl  1408.18\n",
      "| epoch   2 | 18000/18571 batches | lr 1.35 | ms/batch 21.79 | loss  7.25 | ppl  1404.64\n",
      "| epoch   2 |  1000/15143 batches | lr 1.35 | ms/batch 22.28 | loss  7.30 | ppl  1481.45\n",
      "| epoch   2 |  2000/15143 batches | lr 1.35 | ms/batch 21.93 | loss  7.26 | ppl  1425.26\n",
      "| epoch   2 |  3000/15143 batches | lr 1.35 | ms/batch 21.88 | loss  7.27 | ppl  1443.54\n",
      "| epoch   2 |  4000/15143 batches | lr 1.35 | ms/batch 22.13 | loss  7.28 | ppl  1450.27\n",
      "| epoch   2 |  5000/15143 batches | lr 1.35 | ms/batch 21.91 | loss  7.21 | ppl  1350.08\n",
      "| epoch   2 |  6000/15143 batches | lr 1.35 | ms/batch 22.14 | loss  7.29 | ppl  1468.15\n",
      "| epoch   2 |  7000/15143 batches | lr 1.35 | ms/batch 21.97 | loss  7.27 | ppl  1440.31\n",
      "| epoch   2 |  8000/15143 batches | lr 1.35 | ms/batch 21.91 | loss  7.27 | ppl  1440.50\n",
      "| epoch   2 |  9000/15143 batches | lr 1.35 | ms/batch 22.07 | loss  7.26 | ppl  1428.05\n",
      "| epoch   2 | 10000/15143 batches | lr 1.35 | ms/batch 21.86 | loss  7.30 | ppl  1477.91\n",
      "| epoch   2 | 11000/15143 batches | lr 1.35 | ms/batch 21.85 | loss  7.29 | ppl  1470.25\n",
      "| epoch   2 | 12000/15143 batches | lr 1.35 | ms/batch 22.19 | loss  7.30 | ppl  1481.70\n",
      "| epoch   2 | 13000/15143 batches | lr 1.35 | ms/batch 21.83 | loss  7.30 | ppl  1472.95\n",
      "| epoch   2 | 14000/15143 batches | lr 1.35 | ms/batch 22.00 | loss  7.29 | ppl  1459.63\n",
      "| epoch   2 | 15000/15143 batches | lr 1.35 | ms/batch 21.98 | loss  7.31 | ppl  1497.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 773.38s | valid loss  7.52 | valid ppl  1846.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  1000/18571 batches | lr 1.29 | ms/batch 22.28 | loss  7.27 | ppl  1436.02\n",
      "| epoch   3 |  2000/18571 batches | lr 1.29 | ms/batch 21.83 | loss  7.22 | ppl  1364.49\n",
      "| epoch   3 |  3000/18571 batches | lr 1.29 | ms/batch 22.46 | loss  7.24 | ppl  1391.89\n",
      "| epoch   3 |  4000/18571 batches | lr 1.29 | ms/batch 21.87 | loss  7.24 | ppl  1397.52\n",
      "| epoch   3 |  5000/18571 batches | lr 1.29 | ms/batch 21.83 | loss  7.21 | ppl  1350.83\n",
      "| epoch   3 |  6000/18571 batches | lr 1.29 | ms/batch 22.41 | loss  7.22 | ppl  1364.63\n",
      "| epoch   3 |  7000/18571 batches | lr 1.29 | ms/batch 21.75 | loss  7.18 | ppl  1312.42\n",
      "| epoch   3 |  8000/18571 batches | lr 1.29 | ms/batch 21.79 | loss  7.20 | ppl  1335.64\n",
      "| epoch   3 |  9000/18571 batches | lr 1.29 | ms/batch 22.32 | loss  7.21 | ppl  1353.48\n",
      "| epoch   3 | 10000/18571 batches | lr 1.29 | ms/batch 21.70 | loss  7.20 | ppl  1334.50\n",
      "| epoch   3 | 11000/18571 batches | lr 1.29 | ms/batch 22.01 | loss  7.20 | ppl  1344.10\n",
      "| epoch   3 | 12000/18571 batches | lr 1.29 | ms/batch 22.08 | loss  7.22 | ppl  1368.12\n",
      "| epoch   3 | 13000/18571 batches | lr 1.29 | ms/batch 21.78 | loss  7.19 | ppl  1319.75\n",
      "| epoch   3 | 14000/18571 batches | lr 1.29 | ms/batch 22.21 | loss  7.18 | ppl  1309.90\n",
      "| epoch   3 | 15000/18571 batches | lr 1.29 | ms/batch 21.95 | loss  7.21 | ppl  1352.41\n",
      "| epoch   3 | 16000/18571 batches | lr 1.29 | ms/batch 21.78 | loss  7.19 | ppl  1327.10\n",
      "| epoch   3 | 17000/18571 batches | lr 1.29 | ms/batch 22.40 | loss  7.20 | ppl  1339.69\n",
      "| epoch   3 | 18000/18571 batches | lr 1.29 | ms/batch 21.93 | loss  7.19 | ppl  1332.11\n",
      "| epoch   3 |  1000/15143 batches | lr 1.29 | ms/batch 22.35 | loss  7.25 | ppl  1405.76\n",
      "| epoch   3 |  2000/15143 batches | lr 1.29 | ms/batch 21.89 | loss  7.21 | ppl  1354.27\n",
      "| epoch   3 |  3000/15143 batches | lr 1.29 | ms/batch 21.72 | loss  7.22 | ppl  1369.63\n",
      "| epoch   3 |  4000/15143 batches | lr 1.29 | ms/batch 22.19 | loss  7.23 | ppl  1378.88\n",
      "| epoch   3 |  5000/15143 batches | lr 1.29 | ms/batch 21.90 | loss  7.15 | ppl  1280.26\n",
      "| epoch   3 |  6000/15143 batches | lr 1.29 | ms/batch 21.82 | loss  7.24 | ppl  1393.42\n",
      "| epoch   3 |  7000/15143 batches | lr 1.29 | ms/batch 22.28 | loss  7.22 | ppl  1368.13\n",
      "| epoch   3 |  8000/15143 batches | lr 1.29 | ms/batch 21.82 | loss  7.22 | ppl  1370.48\n",
      "| epoch   3 |  9000/15143 batches | lr 1.29 | ms/batch 22.06 | loss  7.21 | ppl  1356.61\n",
      "| epoch   3 | 10000/15143 batches | lr 1.29 | ms/batch 22.01 | loss  7.25 | ppl  1406.14\n",
      "| epoch   3 | 11000/15143 batches | lr 1.29 | ms/batch 21.86 | loss  7.24 | ppl  1398.48\n",
      "| epoch   3 | 12000/15143 batches | lr 1.29 | ms/batch 22.12 | loss  7.25 | ppl  1407.16\n",
      "| epoch   3 | 13000/15143 batches | lr 1.29 | ms/batch 21.85 | loss  7.24 | ppl  1396.75\n",
      "| epoch   3 | 14000/15143 batches | lr 1.29 | ms/batch 21.85 | loss  7.24 | ppl  1389.58\n",
      "| epoch   3 | 15000/15143 batches | lr 1.29 | ms/batch 22.52 | loss  7.26 | ppl  1422.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 774.28s | valid loss  7.54 | valid ppl  1884.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  1000/18571 batches | lr 1.22 | ms/batch 22.26 | loss  7.24 | ppl  1398.71\n",
      "| epoch   4 |  2000/18571 batches | lr 1.22 | ms/batch 21.92 | loss  7.20 | ppl  1340.50\n",
      "| epoch   4 |  3000/18571 batches | lr 1.22 | ms/batch 21.91 | loss  7.22 | ppl  1366.43\n",
      "| epoch   4 |  4000/18571 batches | lr 1.22 | ms/batch 21.94 | loss  7.22 | ppl  1373.26\n",
      "| epoch   4 |  5000/18571 batches | lr 1.22 | ms/batch 21.87 | loss  7.19 | ppl  1330.63\n",
      "| epoch   4 |  6000/18571 batches | lr 1.22 | ms/batch 22.14 | loss  7.20 | ppl  1342.01\n",
      "| epoch   4 |  7000/18571 batches | lr 1.22 | ms/batch 21.98 | loss  7.16 | ppl  1291.29\n",
      "| epoch   4 |  8000/18571 batches | lr 1.22 | ms/batch 21.74 | loss  7.18 | ppl  1310.52\n",
      "| epoch   4 |  9000/18571 batches | lr 1.22 | ms/batch 22.24 | loss  7.19 | ppl  1327.81\n",
      "| epoch   4 | 10000/18571 batches | lr 1.22 | ms/batch 21.85 | loss  7.18 | ppl  1307.52\n",
      "| epoch   4 | 11000/18571 batches | lr 1.22 | ms/batch 21.93 | loss  7.18 | ppl  1315.61\n",
      "| epoch   4 | 12000/18571 batches | lr 1.22 | ms/batch 22.85 | loss  7.20 | ppl  1338.38\n",
      "| epoch   4 | 13000/18571 batches | lr 1.22 | ms/batch 22.34 | loss  7.16 | ppl  1290.02\n",
      "| epoch   4 | 14000/18571 batches | lr 1.22 | ms/batch 22.43 | loss  7.15 | ppl  1279.93\n",
      "| epoch   4 | 15000/18571 batches | lr 1.22 | ms/batch 22.35 | loss  7.19 | ppl  1319.55\n",
      "| epoch   4 | 16000/18571 batches | lr 1.22 | ms/batch 22.27 | loss  7.17 | ppl  1293.86\n",
      "| epoch   4 | 17000/18571 batches | lr 1.22 | ms/batch 22.67 | loss  7.18 | ppl  1306.99\n",
      "| epoch   4 | 18000/18571 batches | lr 1.22 | ms/batch 22.06 | loss  7.17 | ppl  1298.31\n",
      "| epoch   4 |  1000/15143 batches | lr 1.22 | ms/batch 22.39 | loss  7.22 | ppl  1370.27\n",
      "| epoch   4 |  2000/15143 batches | lr 1.22 | ms/batch 22.34 | loss  7.19 | ppl  1320.43\n",
      "| epoch   4 |  3000/15143 batches | lr 1.22 | ms/batch 22.26 | loss  7.20 | ppl  1333.88\n",
      "| epoch   4 |  4000/15143 batches | lr 1.22 | ms/batch 22.63 | loss  7.20 | ppl  1343.91\n",
      "| epoch   4 |  5000/15143 batches | lr 1.22 | ms/batch 22.14 | loss  7.13 | ppl  1245.35\n",
      "| epoch   4 |  6000/15143 batches | lr 1.22 | ms/batch 22.33 | loss  7.21 | ppl  1354.85\n",
      "| epoch   4 |  7000/15143 batches | lr 1.22 | ms/batch 22.64 | loss  7.19 | ppl  1331.13\n",
      "| epoch   4 |  8000/15143 batches | lr 1.22 | ms/batch 22.20 | loss  7.20 | ppl  1333.30\n",
      "| epoch   4 |  9000/15143 batches | lr 1.22 | ms/batch 22.73 | loss  7.19 | ppl  1319.53\n",
      "| epoch   4 | 10000/15143 batches | lr 1.22 | ms/batch 22.24 | loss  7.22 | ppl  1368.16\n",
      "| epoch   4 | 11000/15143 batches | lr 1.22 | ms/batch 21.95 | loss  7.22 | ppl  1359.81\n",
      "| epoch   4 | 12000/15143 batches | lr 1.22 | ms/batch 22.69 | loss  7.22 | ppl  1366.60\n",
      "| epoch   4 | 13000/15143 batches | lr 1.22 | ms/batch 22.06 | loss  7.21 | ppl  1355.24\n",
      "| epoch   4 | 14000/15143 batches | lr 1.22 | ms/batch 21.95 | loss  7.21 | ppl  1351.74\n",
      "| epoch   4 | 15000/15143 batches | lr 1.22 | ms/batch 22.55 | loss  7.23 | ppl  1381.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 782.02s | valid loss  7.57 | valid ppl  1934.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |  1000/18571 batches | lr 1.16 | ms/batch 22.52 | loss  7.23 | ppl  1376.52\n",
      "| epoch   5 |  2000/18571 batches | lr 1.16 | ms/batch 22.31 | loss  7.19 | ppl  1324.30\n",
      "| epoch   5 |  3000/18571 batches | lr 1.16 | ms/batch 22.37 | loss  7.21 | ppl  1351.98\n",
      "| epoch   5 |  4000/18571 batches | lr 1.16 | ms/batch 22.58 | loss  7.22 | ppl  1361.15\n",
      "| epoch   5 |  5000/18571 batches | lr 1.16 | ms/batch 22.08 | loss  7.19 | ppl  1320.38\n",
      "| epoch   5 |  6000/18571 batches | lr 1.16 | ms/batch 22.32 | loss  7.20 | ppl  1333.29\n",
      "| epoch   5 |  7000/18571 batches | lr 1.16 | ms/batch 22.27 | loss  7.16 | ppl  1284.08\n",
      "| epoch   5 |  8000/18571 batches | lr 1.16 | ms/batch 22.23 | loss  7.17 | ppl  1302.35\n",
      "| epoch   5 |  9000/18571 batches | lr 1.16 | ms/batch 22.53 | loss  7.19 | ppl  1320.05\n",
      "| epoch   5 | 10000/18571 batches | lr 1.16 | ms/batch 21.93 | loss  7.17 | ppl  1299.13\n",
      "| epoch   5 | 11000/18571 batches | lr 1.16 | ms/batch 22.40 | loss  7.18 | ppl  1307.11\n",
      "| epoch   5 | 12000/18571 batches | lr 1.16 | ms/batch 22.54 | loss  7.19 | ppl  1328.99\n",
      "| epoch   5 | 13000/18571 batches | lr 1.16 | ms/batch 22.13 | loss  7.15 | ppl  1280.26\n",
      "| epoch   5 | 14000/18571 batches | lr 1.16 | ms/batch 22.56 | loss  7.15 | ppl  1269.70\n",
      "| epoch   5 | 15000/18571 batches | lr 1.16 | ms/batch 22.45 | loss  7.18 | ppl  1307.68\n",
      "| epoch   5 | 16000/18571 batches | lr 1.16 | ms/batch 22.27 | loss  7.16 | ppl  1280.82\n",
      "| epoch   5 | 17000/18571 batches | lr 1.16 | ms/batch 22.42 | loss  7.17 | ppl  1294.90\n",
      "| epoch   5 | 18000/18571 batches | lr 1.16 | ms/batch 22.28 | loss  7.16 | ppl  1285.56\n",
      "| epoch   5 |  1000/15143 batches | lr 1.16 | ms/batch 22.59 | loss  7.21 | ppl  1356.61\n",
      "| epoch   5 |  2000/15143 batches | lr 1.16 | ms/batch 22.35 | loss  7.17 | ppl  1305.05\n",
      "| epoch   5 |  3000/15143 batches | lr 1.16 | ms/batch 22.10 | loss  7.18 | ppl  1318.29\n",
      "| epoch   5 |  4000/15143 batches | lr 1.16 | ms/batch 22.81 | loss  7.19 | ppl  1327.92\n",
      "| epoch   5 |  5000/15143 batches | lr 1.16 | ms/batch 22.21 | loss  7.11 | ppl  1229.14\n",
      "| epoch   5 |  6000/15143 batches | lr 1.16 | ms/batch 22.66 | loss  7.20 | ppl  1337.27\n",
      "| epoch   5 |  7000/15143 batches | lr 1.16 | ms/batch 22.32 | loss  7.18 | ppl  1314.74\n",
      "| epoch   5 |  8000/15143 batches | lr 1.16 | ms/batch 22.28 | loss  7.18 | ppl  1315.39\n",
      "| epoch   5 |  9000/15143 batches | lr 1.16 | ms/batch 22.69 | loss  7.17 | ppl  1300.87\n",
      "| epoch   5 | 10000/15143 batches | lr 1.16 | ms/batch 22.12 | loss  7.21 | ppl  1349.24\n",
      "| epoch   5 | 11000/15143 batches | lr 1.16 | ms/batch 22.26 | loss  7.20 | ppl  1338.96\n",
      "| epoch   5 | 12000/15143 batches | lr 1.16 | ms/batch 22.68 | loss  7.20 | ppl  1344.67\n",
      "| epoch   5 | 13000/15143 batches | lr 1.16 | ms/batch 22.16 | loss  7.20 | ppl  1333.05\n",
      "| epoch   5 | 14000/15143 batches | lr 1.16 | ms/batch 22.67 | loss  7.19 | ppl  1331.63\n",
      "| epoch   5 | 15000/15143 batches | lr 1.16 | ms/batch 22.34 | loss  7.21 | ppl  1357.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 786.83s | valid loss  7.58 | valid ppl  1948.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |  1000/18571 batches | lr 1.10 | ms/batch 22.14 | loss  7.22 | ppl  1364.45\n",
      "| epoch   6 |  2000/18571 batches | lr 1.10 | ms/batch 22.07 | loss  7.18 | ppl  1317.88\n",
      "| epoch   6 |  3000/18571 batches | lr 1.10 | ms/batch 22.76 | loss  7.21 | ppl  1347.22\n",
      "| epoch   6 |  4000/18571 batches | lr 1.10 | ms/batch 22.14 | loss  7.21 | ppl  1358.97\n",
      "| epoch   6 |  5000/18571 batches | lr 1.10 | ms/batch 22.18 | loss  7.18 | ppl  1318.27\n",
      "| epoch   6 |  6000/18571 batches | lr 1.10 | ms/batch 22.78 | loss  7.19 | ppl  1331.62\n",
      "| epoch   6 |  7000/18571 batches | lr 1.10 | ms/batch 21.94 | loss  7.16 | ppl  1284.10\n",
      "| epoch   6 |  8000/18571 batches | lr 1.10 | ms/batch 22.36 | loss  7.17 | ppl  1301.17\n",
      "| epoch   6 |  9000/18571 batches | lr 1.10 | ms/batch 22.40 | loss  7.18 | ppl  1319.15\n",
      "| epoch   6 | 10000/18571 batches | lr 1.10 | ms/batch 22.05 | loss  7.17 | ppl  1297.80\n",
      "| epoch   6 | 11000/18571 batches | lr 1.10 | ms/batch 22.12 | loss  7.17 | ppl  1306.32\n",
      "| epoch   6 | 12000/18571 batches | lr 1.10 | ms/batch 21.92 | loss  7.19 | ppl  1326.15\n",
      "| epoch   6 | 13000/18571 batches | lr 1.10 | ms/batch 21.93 | loss  7.15 | ppl  1277.70\n",
      "| epoch   6 | 14000/18571 batches | lr 1.10 | ms/batch 22.08 | loss  7.14 | ppl  1267.10\n",
      "| epoch   6 | 15000/18571 batches | lr 1.10 | ms/batch 21.84 | loss  7.17 | ppl  1303.25\n",
      "| epoch   6 | 16000/18571 batches | lr 1.10 | ms/batch 21.81 | loss  7.15 | ppl  1275.60\n",
      "| epoch   6 | 17000/18571 batches | lr 1.10 | ms/batch 22.14 | loss  7.16 | ppl  1290.38\n",
      "| epoch   6 | 18000/18571 batches | lr 1.10 | ms/batch 21.82 | loss  7.16 | ppl  1280.69\n",
      "| epoch   6 |  1000/15143 batches | lr 1.10 | ms/batch 22.37 | loss  7.21 | ppl  1351.21\n",
      "| epoch   6 |  2000/15143 batches | lr 1.10 | ms/batch 21.87 | loss  7.17 | ppl  1297.88\n",
      "| epoch   6 |  3000/15143 batches | lr 1.10 | ms/batch 21.89 | loss  7.18 | ppl  1311.75\n",
      "| epoch   6 |  4000/15143 batches | lr 1.10 | ms/batch 22.21 | loss  7.19 | ppl  1320.39\n",
      "| epoch   6 |  5000/15143 batches | lr 1.10 | ms/batch 21.86 | loss  7.11 | ppl  1221.10\n",
      "| epoch   6 |  6000/15143 batches | lr 1.10 | ms/batch 22.20 | loss  7.19 | ppl  1328.27\n",
      "| epoch   6 |  7000/15143 batches | lr 1.10 | ms/batch 21.88 | loss  7.18 | ppl  1307.11\n",
      "| epoch   6 |  8000/15143 batches | lr 1.10 | ms/batch 21.88 | loss  7.17 | ppl  1305.69\n",
      "| epoch   6 |  9000/15143 batches | lr 1.10 | ms/batch 22.38 | loss  7.16 | ppl  1290.82\n",
      "| epoch   6 | 10000/15143 batches | lr 1.10 | ms/batch 21.79 | loss  7.20 | ppl  1338.99\n",
      "| epoch   6 | 11000/15143 batches | lr 1.10 | ms/batch 21.84 | loss  7.19 | ppl  1326.99\n",
      "| epoch   6 | 12000/15143 batches | lr 1.10 | ms/batch 22.32 | loss  7.19 | ppl  1332.59\n",
      "| epoch   6 | 13000/15143 batches | lr 1.10 | ms/batch 21.77 | loss  7.19 | ppl  1320.01\n",
      "| epoch   6 | 14000/15143 batches | lr 1.10 | ms/batch 22.03 | loss  7.19 | ppl  1320.35\n",
      "| epoch   6 | 15000/15143 batches | lr 1.10 | ms/batch 22.01 | loss  7.20 | ppl  1343.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 776.60s | valid loss  7.58 | valid ppl  1963.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |  1000/18571 batches | lr 1.05 | ms/batch 22.32 | loss  7.21 | ppl  1355.87\n",
      "| epoch   7 |  2000/18571 batches | lr 1.05 | ms/batch 21.77 | loss  7.18 | ppl  1314.79\n",
      "| epoch   7 |  3000/18571 batches | lr 1.05 | ms/batch 22.12 | loss  7.20 | ppl  1344.79\n",
      "| epoch   7 |  4000/18571 batches | lr 1.05 | ms/batch 22.03 | loss  7.21 | ppl  1359.61\n",
      "| epoch   7 |  5000/18571 batches | lr 1.05 | ms/batch 21.89 | loss  7.18 | ppl  1319.04\n",
      "| epoch   7 |  6000/18571 batches | lr 1.05 | ms/batch 22.18 | loss  7.19 | ppl  1332.61\n",
      "| epoch   7 |  7000/18571 batches | lr 1.05 | ms/batch 22.00 | loss  7.16 | ppl  1285.87\n",
      "| epoch   7 |  8000/18571 batches | lr 1.05 | ms/batch 21.77 | loss  7.17 | ppl  1302.93\n",
      "| epoch   7 |  9000/18571 batches | lr 1.05 | ms/batch 22.24 | loss  7.19 | ppl  1320.97\n",
      "| epoch   7 | 10000/18571 batches | lr 1.05 | ms/batch 21.84 | loss  7.17 | ppl  1299.28\n",
      "| epoch   7 | 11000/18571 batches | lr 1.05 | ms/batch 21.96 | loss  7.18 | ppl  1308.18\n",
      "| epoch   7 | 12000/18571 batches | lr 1.05 | ms/batch 22.12 | loss  7.19 | ppl  1327.00\n",
      "| epoch   7 | 13000/18571 batches | lr 1.05 | ms/batch 21.86 | loss  7.15 | ppl  1278.20\n",
      "| epoch   7 | 14000/18571 batches | lr 1.05 | ms/batch 22.21 | loss  7.14 | ppl  1267.24\n",
      "| epoch   7 | 15000/18571 batches | lr 1.05 | ms/batch 21.92 | loss  7.17 | ppl  1302.37\n",
      "| epoch   7 | 16000/18571 batches | lr 1.05 | ms/batch 21.86 | loss  7.15 | ppl  1275.32\n",
      "| epoch   7 | 17000/18571 batches | lr 1.05 | ms/batch 22.10 | loss  7.16 | ppl  1288.74\n",
      "| epoch   7 | 18000/18571 batches | lr 1.05 | ms/batch 22.02 | loss  7.15 | ppl  1279.81\n",
      "| epoch   7 |  1000/15143 batches | lr 1.05 | ms/batch 22.19 | loss  7.21 | ppl  1349.85\n",
      "| epoch   7 |  2000/15143 batches | lr 1.05 | ms/batch 21.81 | loss  7.17 | ppl  1294.46\n",
      "| epoch   7 |  3000/15143 batches | lr 1.05 | ms/batch 21.83 | loss  7.18 | ppl  1308.90\n",
      "| epoch   7 |  4000/15143 batches | lr 1.05 | ms/batch 22.17 | loss  7.18 | ppl  1317.68\n",
      "| epoch   7 |  5000/15143 batches | lr 1.05 | ms/batch 21.81 | loss  7.10 | ppl  1217.11\n",
      "| epoch   7 |  6000/15143 batches | lr 1.05 | ms/batch 21.80 | loss  7.19 | ppl  1324.72\n",
      "| epoch   7 |  7000/15143 batches | lr 1.05 | ms/batch 22.03 | loss  7.17 | ppl  1303.83\n",
      "| epoch   7 |  8000/15143 batches | lr 1.05 | ms/batch 21.88 | loss  7.17 | ppl  1300.69\n",
      "| epoch   7 |  9000/15143 batches | lr 1.05 | ms/batch 22.16 | loss  7.16 | ppl  1286.03\n",
      "| epoch   7 | 10000/15143 batches | lr 1.05 | ms/batch 21.77 | loss  7.20 | ppl  1333.24\n",
      "| epoch   7 | 11000/15143 batches | lr 1.05 | ms/batch 21.84 | loss  7.19 | ppl  1320.32\n",
      "| epoch   7 | 12000/15143 batches | lr 1.05 | ms/batch 22.19 | loss  7.19 | ppl  1325.45\n",
      "| epoch   7 | 13000/15143 batches | lr 1.05 | ms/batch 21.65 | loss  7.18 | ppl  1312.79\n",
      "| epoch   7 | 14000/15143 batches | lr 1.05 | ms/batch 21.86 | loss  7.18 | ppl  1313.63\n",
      "| epoch   7 | 15000/15143 batches | lr 1.05 | ms/batch 22.21 | loss  7.20 | ppl  1333.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 773.24s | valid loss  7.59 | valid ppl  1970.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |  1000/18571 batches | lr 1.00 | ms/batch 22.32 | loss  7.21 | ppl  1350.81\n",
      "| epoch   8 |  2000/18571 batches | lr 1.00 | ms/batch 21.66 | loss  7.18 | ppl  1312.79\n",
      "| epoch   8 |  3000/18571 batches | lr 1.00 | ms/batch 21.83 | loss  7.20 | ppl  1344.77\n",
      "| epoch   8 |  4000/18571 batches | lr 1.00 | ms/batch 22.20 | loss  7.22 | ppl  1362.01\n",
      "| epoch   8 |  5000/18571 batches | lr 1.00 | ms/batch 21.73 | loss  7.19 | ppl  1320.37\n",
      "| epoch   8 |  6000/18571 batches | lr 1.00 | ms/batch 22.02 | loss  7.20 | ppl  1335.31\n",
      "| epoch   8 |  7000/18571 batches | lr 1.00 | ms/batch 22.15 | loss  7.16 | ppl  1289.26\n",
      "| epoch   8 |  8000/18571 batches | lr 1.00 | ms/batch 21.72 | loss  7.17 | ppl  1306.27\n",
      "| epoch   8 |  9000/18571 batches | lr 1.00 | ms/batch 22.35 | loss  7.19 | ppl  1323.51\n",
      "| epoch   8 | 10000/18571 batches | lr 1.00 | ms/batch 21.90 | loss  7.17 | ppl  1302.68\n",
      "| epoch   8 | 11000/18571 batches | lr 1.00 | ms/batch 21.70 | loss  7.18 | ppl  1311.37\n",
      "| epoch   8 | 12000/18571 batches | lr 1.00 | ms/batch 22.14 | loss  7.19 | ppl  1328.69\n",
      "| epoch   8 | 13000/18571 batches | lr 1.00 | ms/batch 21.85 | loss  7.15 | ppl  1279.74\n",
      "| epoch   8 | 14000/18571 batches | lr 1.00 | ms/batch 21.75 | loss  7.15 | ppl  1269.21\n",
      "| epoch   8 | 15000/18571 batches | lr 1.00 | ms/batch 22.25 | loss  7.17 | ppl  1303.25\n",
      "| epoch   8 | 16000/18571 batches | lr 1.00 | ms/batch 21.97 | loss  7.15 | ppl  1277.02\n",
      "| epoch   8 | 17000/18571 batches | lr 1.00 | ms/batch 22.14 | loss  7.16 | ppl  1289.45\n",
      "| epoch   8 | 18000/18571 batches | lr 1.00 | ms/batch 21.77 | loss  7.16 | ppl  1280.51\n",
      "| epoch   8 |  1000/15143 batches | lr 1.00 | ms/batch 21.92 | loss  7.21 | ppl  1350.97\n",
      "| epoch   8 |  2000/15143 batches | lr 1.00 | ms/batch 22.09 | loss  7.16 | ppl  1293.20\n",
      "| epoch   8 |  3000/15143 batches | lr 1.00 | ms/batch 21.80 | loss  7.18 | ppl  1309.00\n",
      "| epoch   8 |  4000/15143 batches | lr 1.00 | ms/batch 22.10 | loss  7.18 | ppl  1317.25\n",
      "| epoch   8 |  5000/15143 batches | lr 1.00 | ms/batch 21.96 | loss  7.10 | ppl  1215.45\n",
      "| epoch   8 |  6000/15143 batches | lr 1.00 | ms/batch 22.04 | loss  7.19 | ppl  1323.54\n",
      "| epoch   8 |  7000/15143 batches | lr 1.00 | ms/batch 22.17 | loss  7.17 | ppl  1303.40\n",
      "| epoch   8 |  8000/15143 batches | lr 1.00 | ms/batch 22.01 | loss  7.17 | ppl  1299.35\n",
      "| epoch   8 |  9000/15143 batches | lr 1.00 | ms/batch 21.90 | loss  7.16 | ppl  1284.33\n",
      "| epoch   8 | 10000/15143 batches | lr 1.00 | ms/batch 22.18 | loss  7.19 | ppl  1330.72\n",
      "| epoch   8 | 11000/15143 batches | lr 1.00 | ms/batch 21.82 | loss  7.18 | ppl  1316.52\n",
      "| epoch   8 | 12000/15143 batches | lr 1.00 | ms/batch 22.19 | loss  7.19 | ppl  1322.31\n",
      "| epoch   8 | 13000/15143 batches | lr 1.00 | ms/batch 22.14 | loss  7.18 | ppl  1308.73\n",
      "| epoch   8 | 14000/15143 batches | lr 1.00 | ms/batch 21.73 | loss  7.18 | ppl  1310.69\n",
      "| epoch   8 | 15000/15143 batches | lr 1.00 | ms/batch 22.24 | loss  7.19 | ppl  1328.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 773.78s | valid loss  7.59 | valid ppl  1974.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |  1000/18571 batches | lr 0.95 | ms/batch 22.24 | loss  7.21 | ppl  1347.12\n",
      "| epoch   9 |  2000/18571 batches | lr 0.95 | ms/batch 21.72 | loss  7.18 | ppl  1312.64\n",
      "| epoch   9 |  3000/18571 batches | lr 0.95 | ms/batch 21.82 | loss  7.21 | ppl  1346.18\n",
      "| epoch   9 |  4000/18571 batches | lr 0.95 | ms/batch 22.39 | loss  7.22 | ppl  1365.60\n",
      "| epoch   9 |  5000/18571 batches | lr 0.95 | ms/batch 21.79 | loss  7.19 | ppl  1323.09\n",
      "| epoch   9 |  6000/18571 batches | lr 0.95 | ms/batch 21.82 | loss  7.20 | ppl  1339.61\n",
      "| epoch   9 |  7000/18571 batches | lr 0.95 | ms/batch 22.28 | loss  7.16 | ppl  1292.98\n",
      "| epoch   9 |  8000/18571 batches | lr 0.95 | ms/batch 21.71 | loss  7.18 | ppl  1309.92\n",
      "| epoch   9 |  9000/18571 batches | lr 0.95 | ms/batch 22.10 | loss  7.19 | ppl  1327.78\n",
      "| epoch   9 | 10000/18571 batches | lr 0.95 | ms/batch 22.02 | loss  7.18 | ppl  1306.68\n",
      "| epoch   9 | 11000/18571 batches | lr 0.95 | ms/batch 21.85 | loss  7.18 | ppl  1315.49\n",
      "| epoch   9 | 12000/18571 batches | lr 0.95 | ms/batch 21.96 | loss  7.19 | ppl  1331.84\n",
      "| epoch   9 | 13000/18571 batches | lr 0.95 | ms/batch 21.82 | loss  7.16 | ppl  1282.61\n",
      "| epoch   9 | 14000/18571 batches | lr 0.95 | ms/batch 22.23 | loss  7.15 | ppl  1272.26\n",
      "| epoch   9 | 15000/18571 batches | lr 0.95 | ms/batch 22.16 | loss  7.17 | ppl  1305.79\n",
      "| epoch   9 | 16000/18571 batches | lr 0.95 | ms/batch 21.84 | loss  7.15 | ppl  1280.46\n",
      "| epoch   9 | 17000/18571 batches | lr 0.95 | ms/batch 21.95 | loss  7.16 | ppl  1291.75\n",
      "| epoch   9 | 18000/18571 batches | lr 0.95 | ms/batch 22.02 | loss  7.16 | ppl  1283.89\n",
      "| epoch   9 |  1000/15143 batches | lr 0.95 | ms/batch 21.81 | loss  7.21 | ppl  1353.61\n",
      "| epoch   9 |  2000/15143 batches | lr 0.95 | ms/batch 22.41 | loss  7.17 | ppl  1293.71\n",
      "| epoch   9 |  3000/15143 batches | lr 0.95 | ms/batch 22.05 | loss  7.18 | ppl  1311.91\n",
      "| epoch   9 |  4000/15143 batches | lr 0.95 | ms/batch 22.20 | loss  7.18 | ppl  1318.51\n",
      "| epoch   9 |  5000/15143 batches | lr 0.95 | ms/batch 22.15 | loss  7.10 | ppl  1216.27\n",
      "| epoch   9 |  6000/15143 batches | lr 0.95 | ms/batch 21.91 | loss  7.19 | ppl  1324.19\n",
      "| epoch   9 |  7000/15143 batches | lr 0.95 | ms/batch 22.30 | loss  7.17 | ppl  1305.07\n",
      "| epoch   9 |  8000/15143 batches | lr 0.95 | ms/batch 21.75 | loss  7.17 | ppl  1299.66\n",
      "| epoch   9 |  9000/15143 batches | lr 0.95 | ms/batch 21.87 | loss  7.16 | ppl  1285.16\n",
      "| epoch   9 | 10000/15143 batches | lr 0.95 | ms/batch 22.25 | loss  7.19 | ppl  1330.54\n",
      "| epoch   9 | 11000/15143 batches | lr 0.95 | ms/batch 21.73 | loss  7.18 | ppl  1315.56\n",
      "| epoch   9 | 12000/15143 batches | lr 0.95 | ms/batch 22.03 | loss  7.19 | ppl  1321.02\n",
      "| epoch   9 | 13000/15143 batches | lr 0.95 | ms/batch 22.48 | loss  7.18 | ppl  1307.54\n",
      "| epoch   9 | 14000/15143 batches | lr 0.95 | ms/batch 21.80 | loss  7.18 | ppl  1309.96\n",
      "| epoch   9 | 15000/15143 batches | lr 0.95 | ms/batch 22.37 | loss  7.19 | ppl  1326.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 774.76s | valid loss  7.59 | valid ppl  1975.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |  1000/18571 batches | lr 0.90 | ms/batch 22.20 | loss  7.20 | ppl  1345.54\n",
      "| epoch  10 |  2000/18571 batches | lr 0.90 | ms/batch 22.01 | loss  7.18 | ppl  1313.74\n",
      "| epoch  10 |  3000/18571 batches | lr 0.90 | ms/batch 21.72 | loss  7.21 | ppl  1348.89\n",
      "| epoch  10 |  4000/18571 batches | lr 0.90 | ms/batch 22.17 | loss  7.22 | ppl  1370.86\n",
      "| epoch  10 |  5000/18571 batches | lr 0.90 | ms/batch 22.07 | loss  7.19 | ppl  1326.58\n",
      "| epoch  10 |  6000/18571 batches | lr 0.90 | ms/batch 21.73 | loss  7.20 | ppl  1344.64\n",
      "| epoch  10 |  7000/18571 batches | lr 0.90 | ms/batch 22.36 | loss  7.17 | ppl  1297.11\n",
      "| epoch  10 |  8000/18571 batches | lr 0.90 | ms/batch 21.96 | loss  7.18 | ppl  1315.39\n",
      "| epoch  10 |  9000/18571 batches | lr 0.90 | ms/batch 21.77 | loss  7.19 | ppl  1332.23\n",
      "| epoch  10 | 10000/18571 batches | lr 0.90 | ms/batch 22.36 | loss  7.18 | ppl  1312.30\n",
      "| epoch  10 | 11000/18571 batches | lr 0.90 | ms/batch 21.94 | loss  7.19 | ppl  1320.35\n",
      "| epoch  10 | 12000/18571 batches | lr 0.90 | ms/batch 22.06 | loss  7.20 | ppl  1336.15\n",
      "| epoch  10 | 13000/18571 batches | lr 0.90 | ms/batch 22.09 | loss  7.16 | ppl  1287.03\n",
      "| epoch  10 | 14000/18571 batches | lr 0.90 | ms/batch 21.79 | loss  7.15 | ppl  1275.43\n",
      "| epoch  10 | 15000/18571 batches | lr 0.90 | ms/batch 22.18 | loss  7.18 | ppl  1308.58\n",
      "| epoch  10 | 16000/18571 batches | lr 0.90 | ms/batch 21.71 | loss  7.16 | ppl  1284.87\n",
      "| epoch  10 | 17000/18571 batches | lr 0.90 | ms/batch 21.86 | loss  7.17 | ppl  1295.38\n",
      "| epoch  10 | 18000/18571 batches | lr 0.90 | ms/batch 22.32 | loss  7.16 | ppl  1287.55\n",
      "| epoch  10 |  1000/15143 batches | lr 0.90 | ms/batch 21.97 | loss  7.21 | ppl  1357.92\n",
      "| epoch  10 |  2000/15143 batches | lr 0.90 | ms/batch 22.30 | loss  7.17 | ppl  1295.46\n",
      "| epoch  10 |  3000/15143 batches | lr 0.90 | ms/batch 22.05 | loss  7.18 | ppl  1315.79\n",
      "| epoch  10 |  4000/15143 batches | lr 0.90 | ms/batch 21.86 | loss  7.19 | ppl  1321.94\n",
      "| epoch  10 |  5000/15143 batches | lr 0.90 | ms/batch 22.12 | loss  7.10 | ppl  1217.79\n",
      "| epoch  10 |  6000/15143 batches | lr 0.90 | ms/batch 21.93 | loss  7.19 | ppl  1326.19\n",
      "| epoch  10 |  7000/15143 batches | lr 0.90 | ms/batch 22.19 | loss  7.18 | ppl  1307.72\n",
      "| epoch  10 |  8000/15143 batches | lr 0.90 | ms/batch 22.06 | loss  7.17 | ppl  1301.68\n",
      "| epoch  10 |  9000/15143 batches | lr 0.90 | ms/batch 21.94 | loss  7.16 | ppl  1287.47\n",
      "| epoch  10 | 10000/15143 batches | lr 0.90 | ms/batch 22.70 | loss  7.19 | ppl  1331.00\n",
      "| epoch  10 | 11000/15143 batches | lr 0.90 | ms/batch 21.66 | loss  7.18 | ppl  1316.09\n",
      "| epoch  10 | 12000/15143 batches | lr 0.90 | ms/batch 21.97 | loss  7.19 | ppl  1322.20\n",
      "| epoch  10 | 13000/15143 batches | lr 0.90 | ms/batch 22.49 | loss  7.18 | ppl  1307.97\n",
      "| epoch  10 | 14000/15143 batches | lr 0.90 | ms/batch 21.87 | loss  7.18 | ppl  1311.50\n",
      "| epoch  10 | 15000/15143 batches | lr 0.90 | ms/batch 21.86 | loss  7.19 | ppl  1325.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 775.56s | valid loss  7.59 | valid ppl  1970.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |  1000/18571 batches | lr 0.85 | ms/batch 21.98 | loss  7.20 | ppl  1345.53\n",
      "| epoch  11 |  2000/18571 batches | lr 0.85 | ms/batch 22.24 | loss  7.18 | ppl  1316.64\n",
      "| epoch  11 |  3000/18571 batches | lr 0.85 | ms/batch 21.74 | loss  7.21 | ppl  1353.11\n",
      "| epoch  11 |  4000/18571 batches | lr 0.85 | ms/batch 22.14 | loss  7.23 | ppl  1375.94\n",
      "| epoch  11 |  5000/18571 batches | lr 0.85 | ms/batch 22.05 | loss  7.19 | ppl  1330.77\n",
      "| epoch  11 |  6000/18571 batches | lr 0.85 | ms/batch 21.73 | loss  7.21 | ppl  1350.97\n",
      "| epoch  11 |  7000/18571 batches | lr 0.85 | ms/batch 22.35 | loss  7.17 | ppl  1302.75\n",
      "| epoch  11 |  8000/18571 batches | lr 0.85 | ms/batch 21.93 | loss  7.19 | ppl  1321.42\n",
      "| epoch  11 |  9000/18571 batches | lr 0.85 | ms/batch 21.97 | loss  7.20 | ppl  1337.51\n",
      "| epoch  11 | 10000/18571 batches | lr 0.85 | ms/batch 22.17 | loss  7.18 | ppl  1318.37\n",
      "| epoch  11 | 11000/18571 batches | lr 0.85 | ms/batch 21.93 | loss  7.19 | ppl  1325.37\n",
      "| epoch  11 | 12000/18571 batches | lr 0.85 | ms/batch 22.19 | loss  7.20 | ppl  1341.17\n",
      "| epoch  11 | 13000/18571 batches | lr 0.85 | ms/batch 21.94 | loss  7.16 | ppl  1292.44\n",
      "| epoch  11 | 14000/18571 batches | lr 0.85 | ms/batch 21.98 | loss  7.16 | ppl  1280.51\n",
      "| epoch  11 | 15000/18571 batches | lr 0.85 | ms/batch 22.27 | loss  7.18 | ppl  1312.95\n",
      "| epoch  11 | 16000/18571 batches | lr 0.85 | ms/batch 21.84 | loss  7.16 | ppl  1290.47\n",
      "| epoch  11 | 17000/18571 batches | lr 0.85 | ms/batch 21.97 | loss  7.17 | ppl  1300.29\n",
      "| epoch  11 | 18000/18571 batches | lr 0.85 | ms/batch 22.29 | loss  7.16 | ppl  1292.16\n",
      "| epoch  11 |  1000/15143 batches | lr 0.85 | ms/batch 21.87 | loss  7.22 | ppl  1362.69\n",
      "| epoch  11 |  2000/15143 batches | lr 0.85 | ms/batch 22.34 | loss  7.17 | ppl  1297.17\n",
      "| epoch  11 |  3000/15143 batches | lr 0.85 | ms/batch 21.77 | loss  7.19 | ppl  1320.04\n",
      "| epoch  11 |  4000/15143 batches | lr 0.85 | ms/batch 21.83 | loss  7.19 | ppl  1325.26\n",
      "| epoch  11 |  5000/15143 batches | lr 0.85 | ms/batch 22.32 | loss  7.11 | ppl  1220.88\n",
      "| epoch  11 |  6000/15143 batches | lr 0.85 | ms/batch 21.74 | loss  7.19 | ppl  1329.62\n",
      "| epoch  11 |  7000/15143 batches | lr 0.85 | ms/batch 21.95 | loss  7.18 | ppl  1311.40\n",
      "| epoch  11 |  8000/15143 batches | lr 0.85 | ms/batch 22.39 | loss  7.17 | ppl  1304.84\n",
      "| epoch  11 |  9000/15143 batches | lr 0.85 | ms/batch 21.73 | loss  7.16 | ppl  1290.81\n",
      "| epoch  11 | 10000/15143 batches | lr 0.85 | ms/batch 22.22 | loss  7.20 | ppl  1332.97\n",
      "| epoch  11 | 11000/15143 batches | lr 0.85 | ms/batch 21.91 | loss  7.18 | ppl  1317.84\n",
      "| epoch  11 | 12000/15143 batches | lr 0.85 | ms/batch 21.70 | loss  7.19 | ppl  1324.70\n",
      "| epoch  11 | 13000/15143 batches | lr 0.85 | ms/batch 22.28 | loss  7.18 | ppl  1309.28\n",
      "| epoch  11 | 14000/15143 batches | lr 0.85 | ms/batch 21.87 | loss  7.18 | ppl  1313.60\n",
      "| epoch  11 | 15000/15143 batches | lr 0.85 | ms/batch 21.70 | loss  7.19 | ppl  1326.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 774.65s | valid loss  7.58 | valid ppl  1961.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |  1000/18571 batches | lr 0.81 | ms/batch 21.91 | loss  7.21 | ppl  1346.22\n",
      "| epoch  12 |  2000/18571 batches | lr 0.81 | ms/batch 22.23 | loss  7.19 | ppl  1319.63\n",
      "| epoch  12 |  3000/18571 batches | lr 0.81 | ms/batch 21.82 | loss  7.21 | ppl  1357.51\n",
      "| epoch  12 |  4000/18571 batches | lr 0.81 | ms/batch 21.84 | loss  7.23 | ppl  1381.91\n",
      "| epoch  12 |  5000/18571 batches | lr 0.81 | ms/batch 22.41 | loss  7.20 | ppl  1335.49\n",
      "| epoch  12 |  6000/18571 batches | lr 0.81 | ms/batch 21.97 | loss  7.21 | ppl  1356.97\n",
      "| epoch  12 |  7000/18571 batches | lr 0.81 | ms/batch 22.30 | loss  7.18 | ppl  1308.77\n",
      "| epoch  12 |  8000/18571 batches | lr 0.81 | ms/batch 22.08 | loss  7.19 | ppl  1327.91\n",
      "| epoch  12 |  9000/18571 batches | lr 0.81 | ms/batch 21.87 | loss  7.20 | ppl  1343.96\n",
      "| epoch  12 | 10000/18571 batches | lr 0.81 | ms/batch 22.29 | loss  7.19 | ppl  1324.48\n",
      "| epoch  12 | 11000/18571 batches | lr 0.81 | ms/batch 21.94 | loss  7.19 | ppl  1331.44\n",
      "| epoch  12 | 12000/18571 batches | lr 0.81 | ms/batch 21.89 | loss  7.21 | ppl  1347.14\n",
      "| epoch  12 | 13000/18571 batches | lr 0.81 | ms/batch 22.09 | loss  7.17 | ppl  1297.79\n",
      "| epoch  12 | 14000/18571 batches | lr 0.81 | ms/batch 21.98 | loss  7.16 | ppl  1285.45\n",
      "| epoch  12 | 15000/18571 batches | lr 0.81 | ms/batch 22.09 | loss  7.18 | ppl  1317.89\n",
      "| epoch  12 | 16000/18571 batches | lr 0.81 | ms/batch 22.07 | loss  7.17 | ppl  1296.44\n",
      "| epoch  12 | 17000/18571 batches | lr 0.81 | ms/batch 21.89 | loss  7.17 | ppl  1305.30\n",
      "| epoch  12 | 18000/18571 batches | lr 0.81 | ms/batch 22.47 | loss  7.17 | ppl  1297.34\n",
      "| epoch  12 |  1000/15143 batches | lr 0.81 | ms/batch 21.89 | loss  7.22 | ppl  1368.56\n",
      "| epoch  12 |  2000/15143 batches | lr 0.81 | ms/batch 22.08 | loss  7.17 | ppl  1300.49\n",
      "| epoch  12 |  3000/15143 batches | lr 0.81 | ms/batch 22.05 | loss  7.19 | ppl  1324.56\n",
      "| epoch  12 |  4000/15143 batches | lr 0.81 | ms/batch 21.91 | loss  7.19 | ppl  1330.84\n",
      "| epoch  12 |  5000/15143 batches | lr 0.81 | ms/batch 21.99 | loss  7.11 | ppl  1225.09\n",
      "| epoch  12 |  6000/15143 batches | lr 0.81 | ms/batch 22.09 | loss  7.20 | ppl  1333.39\n",
      "| epoch  12 |  7000/15143 batches | lr 0.81 | ms/batch 21.97 | loss  7.18 | ppl  1315.94\n",
      "| epoch  12 |  8000/15143 batches | lr 0.81 | ms/batch 22.13 | loss  7.18 | ppl  1308.37\n",
      "| epoch  12 |  9000/15143 batches | lr 0.81 | ms/batch 21.91 | loss  7.17 | ppl  1295.35\n",
      "| epoch  12 | 10000/15143 batches | lr 0.81 | ms/batch 22.18 | loss  7.20 | ppl  1335.77\n",
      "| epoch  12 | 11000/15143 batches | lr 0.81 | ms/batch 22.05 | loss  7.19 | ppl  1320.59\n",
      "| epoch  12 | 12000/15143 batches | lr 0.81 | ms/batch 21.98 | loss  7.19 | ppl  1328.12\n",
      "| epoch  12 | 13000/15143 batches | lr 0.81 | ms/batch 22.13 | loss  7.18 | ppl  1312.05\n",
      "| epoch  12 | 14000/15143 batches | lr 0.81 | ms/batch 21.78 | loss  7.18 | ppl  1316.95\n",
      "| epoch  12 | 15000/15143 batches | lr 0.81 | ms/batch 22.00 | loss  7.19 | ppl  1327.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 775.91s | valid loss  7.58 | valid ppl  1952.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |  1000/18571 batches | lr 0.77 | ms/batch 21.84 | loss  7.21 | ppl  1348.24\n",
      "| epoch  13 |  2000/18571 batches | lr 0.77 | ms/batch 22.34 | loss  7.19 | ppl  1323.11\n",
      "| epoch  13 |  3000/18571 batches | lr 0.77 | ms/batch 21.76 | loss  7.22 | ppl  1362.98\n",
      "| epoch  13 |  4000/18571 batches | lr 0.77 | ms/batch 21.94 | loss  7.24 | ppl  1388.62\n",
      "| epoch  13 |  5000/18571 batches | lr 0.77 | ms/batch 22.28 | loss  7.20 | ppl  1340.19\n",
      "| epoch  13 |  6000/18571 batches | lr 0.77 | ms/batch 21.80 | loss  7.22 | ppl  1363.79\n",
      "| epoch  13 |  7000/18571 batches | lr 0.77 | ms/batch 22.06 | loss  7.18 | ppl  1314.74\n",
      "| epoch  13 |  8000/18571 batches | lr 0.77 | ms/batch 22.12 | loss  7.20 | ppl  1335.37\n",
      "| epoch  13 |  9000/18571 batches | lr 0.77 | ms/batch 21.85 | loss  7.21 | ppl  1350.68\n",
      "| epoch  13 | 10000/18571 batches | lr 0.77 | ms/batch 22.50 | loss  7.19 | ppl  1331.49\n",
      "| epoch  13 | 11000/18571 batches | lr 0.77 | ms/batch 22.01 | loss  7.20 | ppl  1337.50\n",
      "| epoch  13 | 12000/18571 batches | lr 0.77 | ms/batch 21.87 | loss  7.21 | ppl  1353.59\n",
      "| epoch  13 | 13000/18571 batches | lr 0.77 | ms/batch 22.42 | loss  7.17 | ppl  1303.69\n",
      "| epoch  13 | 14000/18571 batches | lr 0.77 | ms/batch 21.99 | loss  7.16 | ppl  1291.00\n",
      "| epoch  13 | 15000/18571 batches | lr 0.77 | ms/batch 21.92 | loss  7.19 | ppl  1323.13\n",
      "| epoch  13 | 16000/18571 batches | lr 0.77 | ms/batch 22.23 | loss  7.17 | ppl  1302.96\n",
      "| epoch  13 | 17000/18571 batches | lr 0.77 | ms/batch 21.98 | loss  7.18 | ppl  1310.81\n",
      "| epoch  13 | 18000/18571 batches | lr 0.77 | ms/batch 22.31 | loss  7.17 | ppl  1303.63\n",
      "| epoch  13 |  1000/15143 batches | lr 0.77 | ms/batch 22.08 | loss  7.23 | ppl  1374.80\n",
      "| epoch  13 |  2000/15143 batches | lr 0.77 | ms/batch 21.99 | loss  7.17 | ppl  1304.17\n",
      "| epoch  13 |  3000/15143 batches | lr 0.77 | ms/batch 22.17 | loss  7.19 | ppl  1329.77\n",
      "| epoch  13 |  4000/15143 batches | lr 0.77 | ms/batch 21.93 | loss  7.20 | ppl  1336.30\n",
      "| epoch  13 |  5000/15143 batches | lr 0.77 | ms/batch 22.17 | loss  7.11 | ppl  1229.63\n",
      "| epoch  13 |  6000/15143 batches | lr 0.77 | ms/batch 21.96 | loss  7.20 | ppl  1338.80\n",
      "| epoch  13 |  7000/15143 batches | lr 0.77 | ms/batch 21.82 | loss  7.19 | ppl  1321.80\n",
      "| epoch  13 |  8000/15143 batches | lr 0.77 | ms/batch 22.08 | loss  7.18 | ppl  1313.35\n",
      "| epoch  13 |  9000/15143 batches | lr 0.77 | ms/batch 21.86 | loss  7.17 | ppl  1300.51\n",
      "| epoch  13 | 10000/15143 batches | lr 0.77 | ms/batch 21.89 | loss  7.20 | ppl  1339.24\n",
      "| epoch  13 | 11000/15143 batches | lr 0.77 | ms/batch 22.17 | loss  7.19 | ppl  1324.45\n",
      "| epoch  13 | 12000/15143 batches | lr 0.77 | ms/batch 21.73 | loss  7.19 | ppl  1332.66\n",
      "| epoch  13 | 13000/15143 batches | lr 0.77 | ms/batch 22.07 | loss  7.18 | ppl  1315.71\n",
      "| epoch  13 | 14000/15143 batches | lr 0.77 | ms/batch 22.04 | loss  7.19 | ppl  1321.15\n",
      "| epoch  13 | 15000/15143 batches | lr 0.77 | ms/batch 21.68 | loss  7.19 | ppl  1330.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 775.37s | valid loss  7.57 | valid ppl  1945.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |  1000/18571 batches | lr 0.73 | ms/batch 22.08 | loss  7.21 | ppl  1350.54\n",
      "| epoch  14 |  2000/18571 batches | lr 0.73 | ms/batch 22.25 | loss  7.19 | ppl  1326.51\n",
      "| epoch  14 |  3000/18571 batches | lr 0.73 | ms/batch 21.82 | loss  7.22 | ppl  1369.14\n",
      "| epoch  14 |  4000/18571 batches | lr 0.73 | ms/batch 21.86 | loss  7.24 | ppl  1395.11\n",
      "| epoch  14 |  5000/18571 batches | lr 0.73 | ms/batch 22.36 | loss  7.20 | ppl  1345.57\n",
      "| epoch  14 |  6000/18571 batches | lr 0.73 | ms/batch 21.89 | loss  7.22 | ppl  1371.02\n",
      "| epoch  14 |  7000/18571 batches | lr 0.73 | ms/batch 21.78 | loss  7.19 | ppl  1321.29\n",
      "| epoch  14 |  8000/18571 batches | lr 0.73 | ms/batch 22.34 | loss  7.20 | ppl  1343.03\n",
      "| epoch  14 |  9000/18571 batches | lr 0.73 | ms/batch 21.81 | loss  7.21 | ppl  1358.13\n",
      "| epoch  14 | 10000/18571 batches | lr 0.73 | ms/batch 21.97 | loss  7.20 | ppl  1338.17\n",
      "| epoch  14 | 11000/18571 batches | lr 0.73 | ms/batch 21.98 | loss  7.20 | ppl  1344.51\n",
      "| epoch  14 | 12000/18571 batches | lr 0.73 | ms/batch 21.90 | loss  7.22 | ppl  1360.83\n",
      "| epoch  14 | 13000/18571 batches | lr 0.73 | ms/batch 22.03 | loss  7.18 | ppl  1310.75\n",
      "| epoch  14 | 14000/18571 batches | lr 0.73 | ms/batch 21.98 | loss  7.17 | ppl  1296.61\n",
      "| epoch  14 | 15000/18571 batches | lr 0.73 | ms/batch 21.87 | loss  7.19 | ppl  1329.25\n",
      "| epoch  14 | 16000/18571 batches | lr 0.73 | ms/batch 22.05 | loss  7.18 | ppl  1310.71\n",
      "| epoch  14 | 17000/18571 batches | lr 0.73 | ms/batch 22.01 | loss  7.18 | ppl  1317.11\n",
      "| epoch  14 | 18000/18571 batches | lr 0.73 | ms/batch 21.90 | loss  7.18 | ppl  1309.71\n",
      "| epoch  14 |  1000/15143 batches | lr 0.73 | ms/batch 21.98 | loss  7.23 | ppl  1381.08\n",
      "| epoch  14 |  2000/15143 batches | lr 0.73 | ms/batch 21.78 | loss  7.18 | ppl  1308.59\n",
      "| epoch  14 |  3000/15143 batches | lr 0.73 | ms/batch 22.36 | loss  7.20 | ppl  1336.22\n",
      "| epoch  14 |  4000/15143 batches | lr 0.73 | ms/batch 21.85 | loss  7.20 | ppl  1342.15\n",
      "| epoch  14 |  5000/15143 batches | lr 0.73 | ms/batch 22.01 | loss  7.12 | ppl  1234.70\n",
      "| epoch  14 |  6000/15143 batches | lr 0.73 | ms/batch 21.90 | loss  7.20 | ppl  1343.88\n",
      "| epoch  14 |  7000/15143 batches | lr 0.73 | ms/batch 21.84 | loss  7.19 | ppl  1327.67\n",
      "| epoch  14 |  8000/15143 batches | lr 0.73 | ms/batch 22.19 | loss  7.18 | ppl  1318.67\n",
      "| epoch  14 |  9000/15143 batches | lr 0.73 | ms/batch 21.75 | loss  7.17 | ppl  1305.90\n",
      "| epoch  14 | 10000/15143 batches | lr 0.73 | ms/batch 21.88 | loss  7.20 | ppl  1343.28\n",
      "| epoch  14 | 11000/15143 batches | lr 0.73 | ms/batch 22.31 | loss  7.19 | ppl  1328.75\n",
      "| epoch  14 | 12000/15143 batches | lr 0.73 | ms/batch 21.84 | loss  7.20 | ppl  1337.58\n",
      "| epoch  14 | 13000/15143 batches | lr 0.73 | ms/batch 21.92 | loss  7.19 | ppl  1319.80\n",
      "| epoch  14 | 14000/15143 batches | lr 0.73 | ms/batch 22.41 | loss  7.19 | ppl  1326.24\n",
      "| epoch  14 | 15000/15143 batches | lr 0.73 | ms/batch 21.79 | loss  7.20 | ppl  1333.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 775.04s | valid loss  7.57 | valid ppl  1937.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |  1000/18571 batches | lr 0.69 | ms/batch 21.85 | loss  7.21 | ppl  1353.76\n",
      "| epoch  15 |  2000/18571 batches | lr 0.69 | ms/batch 22.26 | loss  7.19 | ppl  1331.05\n",
      "| epoch  15 |  3000/18571 batches | lr 0.69 | ms/batch 22.24 | loss  7.23 | ppl  1375.47\n",
      "| epoch  15 |  4000/18571 batches | lr 0.69 | ms/batch 21.76 | loss  7.25 | ppl  1402.40\n",
      "| epoch  15 |  5000/18571 batches | lr 0.69 | ms/batch 22.24 | loss  7.21 | ppl  1351.19\n",
      "| epoch  15 |  6000/18571 batches | lr 0.69 | ms/batch 21.97 | loss  7.23 | ppl  1378.93\n",
      "| epoch  15 |  7000/18571 batches | lr 0.69 | ms/batch 21.83 | loss  7.19 | ppl  1327.87\n",
      "| epoch  15 |  8000/18571 batches | lr 0.69 | ms/batch 22.37 | loss  7.21 | ppl  1350.95\n",
      "| epoch  15 |  9000/18571 batches | lr 0.69 | ms/batch 21.93 | loss  7.22 | ppl  1365.56\n",
      "| epoch  15 | 10000/18571 batches | lr 0.69 | ms/batch 21.80 | loss  7.21 | ppl  1346.31\n",
      "| epoch  15 | 11000/18571 batches | lr 0.69 | ms/batch 22.49 | loss  7.21 | ppl  1351.33\n",
      "| epoch  15 | 12000/18571 batches | lr 0.69 | ms/batch 22.00 | loss  7.22 | ppl  1368.31\n",
      "| epoch  15 | 13000/18571 batches | lr 0.69 | ms/batch 22.17 | loss  7.18 | ppl  1317.49\n",
      "| epoch  15 | 14000/18571 batches | lr 0.69 | ms/batch 22.07 | loss  7.17 | ppl  1302.76\n",
      "| epoch  15 | 15000/18571 batches | lr 0.69 | ms/batch 21.92 | loss  7.20 | ppl  1335.85\n",
      "| epoch  15 | 16000/18571 batches | lr 0.69 | ms/batch 22.19 | loss  7.18 | ppl  1317.86\n",
      "| epoch  15 | 17000/18571 batches | lr 0.69 | ms/batch 21.92 | loss  7.19 | ppl  1323.12\n",
      "| epoch  15 | 18000/18571 batches | lr 0.69 | ms/batch 21.82 | loss  7.18 | ppl  1317.04\n",
      "| epoch  15 |  1000/15143 batches | lr 0.69 | ms/batch 21.98 | loss  7.24 | ppl  1388.21\n",
      "| epoch  15 |  2000/15143 batches | lr 0.69 | ms/batch 22.08 | loss  7.18 | ppl  1313.51\n",
      "| epoch  15 |  3000/15143 batches | lr 0.69 | ms/batch 22.18 | loss  7.20 | ppl  1342.05\n",
      "| epoch  15 |  4000/15143 batches | lr 0.69 | ms/batch 21.95 | loss  7.21 | ppl  1348.70\n",
      "| epoch  15 |  5000/15143 batches | lr 0.69 | ms/batch 21.94 | loss  7.12 | ppl  1239.77\n",
      "| epoch  15 |  6000/15143 batches | lr 0.69 | ms/batch 22.26 | loss  7.21 | ppl  1349.55\n",
      "| epoch  15 |  7000/15143 batches | lr 0.69 | ms/batch 21.94 | loss  7.20 | ppl  1333.73\n",
      "| epoch  15 |  8000/15143 batches | lr 0.69 | ms/batch 22.19 | loss  7.19 | ppl  1324.02\n",
      "| epoch  15 |  9000/15143 batches | lr 0.69 | ms/batch 21.98 | loss  7.18 | ppl  1312.07\n",
      "| epoch  15 | 10000/15143 batches | lr 0.69 | ms/batch 21.92 | loss  7.21 | ppl  1347.28\n",
      "| epoch  15 | 11000/15143 batches | lr 0.69 | ms/batch 22.42 | loss  7.20 | ppl  1333.79\n",
      "| epoch  15 | 12000/15143 batches | lr 0.69 | ms/batch 21.88 | loss  7.20 | ppl  1343.14\n",
      "| epoch  15 | 13000/15143 batches | lr 0.69 | ms/batch 21.72 | loss  7.19 | ppl  1325.01\n",
      "| epoch  15 | 14000/15143 batches | lr 0.69 | ms/batch 22.45 | loss  7.19 | ppl  1331.37\n",
      "| epoch  15 | 15000/15143 batches | lr 0.69 | ms/batch 21.98 | loss  7.20 | ppl  1338.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 776.50s | valid loss  7.56 | valid ppl  1926.43\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 15\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data1)\n",
    "    train(train_data2)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    val_perplexity = math.exp(val_loss)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, val_perplexity))\n",
    "    print('-' * 89)\n",
    "        \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "        torch.save(best_model.state_dict(), 'best_model' + str(epoch) + '.pth')\n",
    "\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.149849,
     "end_time": "2021-01-24T00:05:00.497154",
     "exception": false,
     "start_time": "2021-01-24T00:05:00.347305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 11855.03679,
   "end_time": "2021-01-24T00:05:01.358877",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-23T20:47:26.322087",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
